\section{Methodology}
\label{sec:meth}
This section is specifically aimed at explaining how the differences in performance between a CNN-classifier and other methods are measured. This entails that the data, the experiments and the algorithms are all examined in order to show how the research question is answered. \\


\subsection{Description of the data}
\label{subsec:data}
The data used within this project consist of two types of data, both from the government of the Netherlands. The first set consists of over 50,000 questions with answers that have been asked within the Dutch national parliament. Each question-answer pair is annotated with a number of labels. Each of the labels then consists of two levels of detail; it has one of the 17 undetailed categories, such as law or education, and one of the 118 more detailed categories, such as criminal law or primary education. Figure \ref{fig:distributiontopics} shows the distributions of topics within the dataset and figure \ref{fig:LabelAmount} shows how many labels each document contains.\\

\begin{figure}[H]
	\begin{center}
		\belowbaseline[0pt]{ \includegraphics[width=.49\linewidth]{TrainLabels118}}~\belowbaseline[0pt]{ \includegraphics[width=.49\linewidth]{TrainLabels17}}
		\caption{Distribution of topics within the parlement data}
		\label{fig:distributiontopics}
	\end{center}
\end{figure}

The questions are collected from www.zoek.officielebekendmakingen.nl with a scraper and the set consist of all question asked from 2001 to 2017. This means all of the labels have been discussed in a wide variety of manners. Moreover, the documents vary on length as can be seen in Figure \ref{fig:WordAmount} and which politicians have asked and answered these questions.\\

\begin{figure}[H]
	\centering
  	\includegraphics[width=.9\linewidth]{TrainAmountLabels}
  	\caption{Amount of labels per document}
  	\label{fig:LabelAmount}
\end{figure}

\begin{figure}[H]
	\centering
  	\includegraphics[width=.9\linewidth]{TrainAmountWords}
  	\caption{Box plots of the amount of words, unique words and characters within the train data.}
  	\label{fig:WordAmount}
\end{figure}

The second part of the data is dat of municipalities retrieved from www.openraadsinformatie.nl. ...............................................................\\


%Data verzameling en beschrijving van de data

%Hoe is de data verzameld, en hoe heb jij die data verkregen?


%Wat staat er in de data? Niet alleen maar een technisch verhaal, maar ook inhoudelijk. DE lezer moet een goed idee krijgen over de technische inhoud en wat het betekent.

\subsection{Algorithms}
The CNN of this research are compared to a number of traditional text classifiers; Support Vector Machines (SVM), Random Forrest (RF) and Multinominal Naive Bayes (NB). For all these implementations the input is a bag-of-words representation of the various documents. For all of these algorithms the implementation of Scikit-Learn is employed and the optimal hyper-parameters are chosen based on a grid-search.\\
The CNN within this research are similar to earlier architectures \cite{kim2014convolutional}. This means that an embedding layer is used to transform sentences to a multi-dimensional space using Word2Vec-embeddings. This research experiments with two embeddings, namely pre-trained embeddings retrieved trainedon a variety of Dutch resources \cite{tulkens2016evaluating} and embeddings trained on the data described in section \ref{subsec:data}. Both embedding spaces have been tested with static and non-static initializations. Thereafter three convolutional layers and three max-pooling layers are alternated between. For the convolutional layers multiple filter sizes have been tested and in addition also multiple filter sizes in one layer are experimented with.\\
Then the multidimensional is flattened and a dropout layer is used to prevent overfitting within the network. In some architectures also L1-regularization has been used, however, later research demonstrated the minimal effect of this regularization \cite{zhang2015sensitivity}. The last two layers are fully connected layers in order to gain the final prediction. Since the data is multilabel binary crossentropy is used as loss and a sigmoid function as the activation. The final prediction is created by rounding the output of the activation to 0 or 1 per label. \\
This research adds to the existing architectures in its method to deal with variable document sizes instead of merely padding and cutting sentences to the input size. Two possibilities have been examined; Firstly, the sentences are split into multiple smaller parts. Each of these individual parts of the sentence are classified individually. Then these predictions aggregated into one prediction, either by summing or using the max value. Then once again these predictions are rounded per label to create a prediction.\\
The second implementation attempts to fix the length discrepency within the embedding stage. The documents are all split in an equal number pieces. These pieces are then embedded using the Par2Vec embeddings \cite{le2014distributed}, which employs a pre-trained embedding space. This method ensures that all the documents have an equal input size for the network, however, for some documents the embeddings represent multiple words or even entire paragraphs. Similarly to the other classifiers, multiple parameters are tested such as input-size, static and non-static initialization and filter sizes. \\ 

\subsection{Experiments}
The various algorithms are evaluated on the basis of two test-sets, both from a different source of data as explained in section \ref{subsec:data}. The first experiment is carried out on the data with question of the Dutch parlement. This set is split into three parts of respectively 70, 20 and 10 percent of the data. The models are firstly trained on 70 percent of the data. Then, the optimal hyperparameters, such as the decision threshold, are chosen by evaluating the performance on the 20 percent of the data. When these parameters have been selected, the final versions is tested with the last 10 percent of the data. This experiment is conducted twice, using both the data with 17 and 118 different topics. Using different amount of topics shows how well algorithms perform depending on the detail of the topics.\\
Within the second experiment the transfer between different datasets is specifically important. The model is trained on 80 percent of the parlement-data and the remaining parlement data is used as validation data to select the optimal parameters. However, this model is evaluated using the manually labelled dataset of the Dutch municipalities in order to see how well the models transfer to another dataset. In contrast to the first experiment this experiment is merely conducted with 17 topics, as the municipality data is only classified that way.\\
Success in both experiments is measured using the micro-average F1-score, which balances the precision and recall of the prediction. However, in addition to the F1-scores also confusion matrices are used in order to evaluate what kind of errors are made. Lastly, properties of documents that are missclassified are evaluated per algorithm to better understand how the algorithms perform on specific types of documents. 


%Hoe je je vraag gaat beantwoorden.
%
%
%Dit is de langste sectie van je scriptie. 
%
%Als iets erg technisch wordt kan je een deel naar de Appendix verplaatsen. 
%
%Probeer er een lopend verhaal van te maken.
%
%Het is heel handig dit ook weer op te delen nav je deelvragen:
%
%\subsubsection{RQ1}
%
%\subsubsection{RQ2}