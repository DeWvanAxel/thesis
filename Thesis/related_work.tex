\section{Related Work}
\label{sec:rel}
Classifcation has been a widely studied information problem and its various solutions are discussed within this section. The goal of classification is to assign one or multiple labels to documents based on its contents. This classification is contingent on labeled train data, which is used to train a classifier in distinquishing between various classes. Often, a number of pre-processing steps are executed, in order to engineer a document representation which contains relevant content. A common representation is the bag-of-words representation, which is used to transform documents into a vector which indicates how many times words of the vocabulary occur within the text.\\
One of the algorithms that uses bag-of-words as input is the miltinominal naive bayes classifier, which uses probabilities of words occuring within a specific topic in order to classify unseen documents. One of the ways to increase performance is to extend the probabilities based on word occurences with other features, such as which organization published a document or on what kind of domain names documents were found \cite{sahami1998bayesian}. Another version, very similar to Naive Bayes, includes document frequency within the bag-of-words vector, and thus divide the word-occurences within that specific document by the amount of documents in which that word occurs \cite{joachims1996probabilistic}. \\
Another frequently used algorithm for text classification is decision tree, which is an algorithm which establishes a hierarcal division based on textual features. These splits are based on features spaces which have a more skewed distribution of the classes, for example document in which certain words occur are often from a certain class. Multiple trees can be created as an ensemble, each on part of the data, to prevent overfitting to the train data and these are called random forest classifiers. Although the algorithm and its outcome are easily understandable its performance is often worse than other methods \cite{li1998classification}.\\
The last method based on the bag-of-words implementation is the SVM, which has been the state-of-the-art for many years. Within SVM hyperplanes are constructed that split datapoints within the multidimensional space. It is argued that SVM can perform well on textual data, since few features are relevant though those which are relevant correlate. This allows SVM classifiers to easily distinquish between various classes \cite{joachims2001statistical}.\\
All these algorithms based on bag-of-words have had specific classification problems in which they have excelled. However, the bag-of-words representation has a number of detriments. Foremost, the representation is unable to see the relation between "strong" and "powerful". The words are equally far apart as any random word such as "flower". Moreover, the representation treats documents as a collection of words instead of an ordered sequence. These two problems refrain algorithms to identify specific patterns and nuances in texts.\\
Recently, new document representations have been developed which use word embeddings. Word embeddings are multi-dimensional vectors that represent the semantical meaning of words. These embeddings are created using a neighbourhood approach, and therefore words that appear in similar contexts also are located closeby within the multi-dimensional space. \cite{mikolov2013efficient} Documents can then be represented by ordering these word-vectors in the same sequence as original sentences.\\



%\subsection{Document representation}
%Documents can be represented in multiple ways when used within text classification. One of these methods is bag-of-words, which means that each document is represented as a vector with the length of the vocabulary. Each entry in that vector represents how many times the corresponding word occurs within that document. \cite{harris1954distributional} Often, this representation is expanded upon with TF-IDF, which includes the inverse document frequency of words as well. Therefore the occurences of words within are scaled based on the total amount of documents in which that word occurs.\\
%A similar representation technique is to represent documents with vectors with a length of the vocabulary size. Within this representation each entry within such a vector is a binary number representing the presence or absence of that word within a document. \\  
%While bag-of-words and these binary vectors have been the standard representation for many years, they do have significant disadvantages. This is foremost caused by the loss of word order within those representations. Also, words with similar meaning, such as "hate" and "loathe" are equally far apart within this representation as words with totally different meaning such as "hate" and "love". This means that this representation is less well equiped to deal with nuanced differences in meaning and context.\\
%Representing documents can also be done using word embeddings. Word embeddings are multi-dimensional vectors that represent the semantical meaning of words. These embeddings are created using a neighbourhood approach, and therefore words that appear in similar contexts also are located closeby within the multi-dimensional space. \cite{mikolov2013efficient} Documents can then be represented by ordering these word-vectors in the same sequence as original sentences.\\
%Using embedding spaces such as Word2Vec are thus not prone to the loss of word order. This means that with these representation also contextual nuances within sentences caused by word order can be understood. This is not possible using only words or bag-of-words, as it is unlikely that specific words occur in the same order within multiple documents.  Moreover, similar sentences are represented by closely related vectors, which allows systems to also understand sentences when other words are used. \\
%PAR2VEC
%
%\subsection{Classifiers}
%\subsubsection{Probalistic models}
%One of the most prominent ways for text classification is the Naive Bayes classifier. The Naive Bayes classifier learns the probability words occur within a certain topic. New documents can be classified using the distribution of words within that document. However, the Naive Bayes assumes that items occur independently of each other, and this assumption often does not hold. Still, the Naive Bayes can perform relatively well. \cite{allahyari2017brief} Two distinct ways of Naive Bayes can be found: Multi-variate Bernoulli model and multinomial model. The main difference is the input for the model. Within the multi-variate Bernoulli the binary vector representation is used and within the multinominal model the bag-of-words is used.\\
%For both models its learning process is relatively similar. For each word it is calculated how many times it occurs in each class. Then this number plus one is divided by the amount of unique words plus the amount of words within documents of that class. This function yields the probability of a document containing that word belongs to a certain class. To classify new documents the probabilities for the words within that document can be calculated. Using these probabilities the relative relative probability of the document to belong to a certain class can be calculated. \cite{mccallum1998comparison}\\
%Although Naive Bayes algorithms are performing quite well they do have some performance issues. It suffers from the problems of the binary and bag-of-words representation. Moreover, the independency assumption also prevents naive bayes from discovering more complex relations within texts, as it is unable to see that co-occurences of specific words can actually define the class of a sentence. \\ 
% 
%\subsubsection{Decision trees}
%Tree-based classifiers classify documents by creating a set of rules based on the presence and absence of words within documents. The training of decision trees is done by splitting the training set into multiple splits. Each individual split is based on the information gain provided, which means that by selecting on the specific feature of this split the greatest amount of extra documents can be classified correctly. To prevent overfitting the data is being split untill each node has a certain minimum of words or untill a maxinum number of splits are performed.
%
%\subsubsection{Linear models}
%\begin{itemize}
%\item SVM classifiers
%
%\item Regression-based classifiers
%
%\end{itemize}
%
%
%\subsubsection{Neural networks}





%Deze sectie bestaat uit een aantal "blokken", waarin je per blok de relevante literatuur beschrijft. 
%
%Neem alleen literatuur op die van belang is voor jouw onderzoeksvraag en deelvragen.
%
%Typisch heb je 1 blok voor je hoofdvraag en per deelvraag \textbf{RQi} een blok. 
%
%
%\subsection{RQ1}
%
%\subsection{RQ2}