\section{Related Work}
\label{sec:rel}
Within this section related research is discussed to show how this research relates to it. The task of classification is discussed first, together with the regularly used classification algorithms. Secondly, it is explained what multilabel classification is and how the algorithms should be adapted. Lastly, the relevance of domain adaptation for this research is examined.

\subsection{Classification}
Within classification a training set \textit{D} of length \textit{N} is present, and each sample has a label from a limited set of labels \textit{L} assigned to it. This data is used to construct a classification model (also known as a classifier) using classification algorithms, which relate features of samples within \textit{D} to labels in \textit{L}. The goal of classification is correctly predict labels for documents based on its content. To evaluate how well the classifiers can do this, a part of \textit{D} is used as test data which the models attempt to predict accurately \cite{aggarwal2012survey}. 

\subsubsection{Bag-of-words classifiers}
All classifiers are thus contingent on labeled train data, which is used to train the classifiers. Often, the data is pre-processed in order to engineer a document representation which contains content which can be used by the classifiers to distinguish relevant features. A common representation is the BoW representation, which is used to transform documents in \textit{D} into vectors which indicate how many times words of the vocabulary occur within the documents \cite{aggarwal2012survey}.\\
One of the algorithms that uses bag-of-words as input is the multinominal naive bayes classifier, which uses probabilities of words occuring within a specific topic in order to classify unseen documents \cite{aggarwal2012survey}.\\
%One of the ways to increase performance is to extend the probabilities based on word occurences with other features, such as which organization published a document or on what kind of domain names documents were found \cite{sahami1998bayesian}. Another version, very similar to Naive Bayes, includes document frequency within the bag-of-words vector, and thus divide the word-occurences within that specific document by the amount of documents in which that word occurs \cite{joachims1996probabilistic}. \\
Another frequently used algorithm for text classification is the decision tree classifier, which is an algorithm that establishes a hierarcal division based on textual features. These splits are based on feature spaces which have a more skewed distribution of the classes. Multiple trees can be created as an ensemble, each on part of the data, to prevent overfitting to the train data and these are called random forest classifiers. Although the algorithm and its outcome are easily understandable its performance is often worse than other methods \cite{li1998classification}.\\
Logistic regression is also used as classifier, and it employs a statistical approach for classification. Its prediction is based on the multiplication of parameters and variables. During training these parameters are optimized with gradient descent in such a way that as many samples are classified correctly with high probability. To predict unseen samples it takes the log of the multiplication, which when rounded ends up as a 0 or 1 \cite{aggarwal2012survey}. \\ 
The last method based on the bag-of-words implementation is the SVM, which has been the state-of-the-art for many years. Within SVM hyperplanes are constructed that split datapoints within the multidimensional space. It is argued that SVM can perform well on textual data, since few features are relevant though those which are relevant correlate. This allows SVM classifiers to easily distinquish between various classes \cite{joachims2001statistical}.\\
Although SVM and logistic regression perform well, they are naturally binary classifiers. This means that instead of having any amount in labels within \textit{L} merely two labels can be used. However, this problem can be easily solved by constructing an one-vs-rest classifier, which produces a new binary model for each label against all other labels. This means that all samples of one label are considered as positive samples and all the other samples without the label as negative samples for the training of the model. Eventually labels are assigned by applying all classifiers to new samples and predicting a label from \textit{L} for the classifier which reports the highest score \cite{aggarwal2012survey}.\\
The algorithms based on bag-of-words have performed well on many classification tasks. However, the bag-of-words representation has a number of detriments. Foremost, the representation is unable to see the relation between "strong" and "powerful" as these words are equally far apart as they are different from any other word such as "flower". Moreover, the representation treats documents as a collection of words instead of an ordered sequence of words. These two problems refrain algorithms to identify specific patterns and nuances in texts.

\subsubsection{Word embedding classifiers}
Recently, new document representations have been developed which use word embeddings. Word embeddings are multidimensional vectors that represent the semantical meaning of words. These embeddings are created using a neighbourhood approach, and therefore words that appear in similar contexts are located near each other within the multidimensional space. \cite{mikolov2013efficient} Documents can then be represented by ordering these word vectors in the same sequence as original sentences.\\
This representation allows successful employment of deep neural networks for text classification, and two distinct types can be distinquished: Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN). Although both types have achieved state-of-the-art results within various natural language processing tasks, CNNs are expected to have better performance on classification tasks where pattern recognition is vital \cite{yin2017comparative}. Moreover, CNNs can be trained faster than RNNs, because it can be parallized on the GPU. Since keyphrases and similar structures seem to be important for this specific task, and because computational speed is important, CNNs have been further explored.\\ 
CNNs employ filters that are applied to local features and this has originally been used for processing image data. However, recent research has shown that CNNs can also be used for natural language processing tasks. Kim et al. (2014) show that CNNs with Word2Vec-embeddings achieve excellent results, which suggests that the embeddings are good feature extractors. Their architecture consist of multiple pooling layers and 1D-convolutional layers and then two fully connected layers which produce the final output. They also experiment with multiple filter sizes in the convolutional layers, which also increases performance. Their last finding is that dynamically adjusting the embeddings boosts performance in many tasks. \\
CNNs are limited to classifying documents with the same length, and most researchers solve this problem with padding and splitting documents. This means that a fixed length is chosen as parameter, and sentences that are longer are cut off at that point.Vectors with only zeros, and thus meaning, are appended to sentences that are shorter in order to get to the fixed length \cite{kim2014convolutional}  \cite{zhang2015sensitivity}. \\
Another approach to deal with different sentence lengths is the Dynamic CNN, which allows different sentence-sizes as input. After convolution layers a dynamic k-max-pooling layer, which computes the amount of features pooled to the next layerly based on the sentence's length. Only within the last pooling layer a fixed number of features is pooled and used within the last dense activation layer \cite{kalchbrenner2014convolutional}.\\
This architecture is only suitable for sentences but has been extendend to entire documents as well. The adapted algorithm uses the output from the last layer of the Dynamic CNN employed on each sentence. Then based on the length of the document these features are then once again used in a Dynamic CNN \cite{denil2014modelling}.

\subsection{Adaptions for multilabel classification}
Much of the multiclass classification challenge is similar for multilabel classification too. However, instead of giving one label from the limited set \textit{L} now any amount of labels can be assigned, including no label at all. This means that the training data \textit{D} also contains samples with varying numbers of labels assigned to it. All previously discussed algorithms can be used for multilabel classification as well when adapted.\\
The outcome of the algorithms is already a predictions between 0 and 1 for each label. So instead of chosing the label with the highest score, a threshold can be defined, such as 0.5. Now samples are considered part of a class if the independent probability of it belonging to that class is higher than the threshold.\\
In addition to this, for CNNs the final activation of the last connected layer should be changed from "softmax" to "sigmoid" as "sigmoid" calculates independent probabilities for each label. Many people also employ a different loss function to optimize multiclass neural networks, such as binary cross-entropy instead of multiclass cross-entropy \cite{nam2014large} \cite{Multiclass}.

\subsection{Domain adaptation}
Transfer learning is the discipline dedicated to transfering models from one task to another. One typical problem of transfer learning is transductive transfer learning or domain adaptation, which formally can be described as learning a task on the source training set $\textit{D}_S$. The goal is to create a model that minimizes the error of the same task on a target test set $\textit{D}_T$ \cite{pan2010survey}. \\
One of the methods to do domain adaptation is a transfer of feature representations, and the use of Word2Vec can already be seen as an example of this type of adaptation \cite{nguyen2015event}. Other examples of feature representation transfers are some dimensionality reduction algorithms employed on the bag-of-words representations and finding domain specific features and removing those \cite{pan2010survey}.\\
Another of the feature representations transfers is the autoencoder, which is an unsupervised neural network. The idea of the autoencoder is to find a lower dimensionlaity space with which you can construct the original sample. However, the local structure is lost during the process, which is why it is less suited for combination with CNNs \cite{ganin2014unsupervised}.\\
Another type of transfer suited for domain adaptation is transferring the knowledge of instances. The most common way to do this is with importance sampling, which implies that training samples in $\textit{D}_S$ that are very alike to test samples in $\textit{D}_T$ are more important during training. This is done by constructing a classifier which recognizes whether samples come from $\textit{D}_S$ or $\textit{D}_T$. During training on $\textit{D}_S$ for the classifier of the eventual task, samples in $\textit{D}_S$ are weighted based on how much the previously constructed classifier predicts them to be part of $\textit{D}_T$ \cite{pan2010survey}. 

%\subsection{Document representation}
%Documents can be represented in multiple ways when used within text classification. One of these methods is bag-of-words, which means that each document is represented as a vector with the length of the vocabulary. Each entry in that vector represents how many times the corresponding word occurs within that document. \cite{harris1954distributional} Often, this representation is expanded upon with TF-IDF, which includes the inverse document frequency of words as well. Therefore the occurences of words within are scaled based on the total amount of documents in which that word occurs.\\
%A similar representation technique is to represent documents with vectors with a length of the vocabulary size. Within this representation each entry within such a vector is a binary number representing the presence or absence of that word within a document. \\  
%While bag-of-words and these binary vectors have been the standard representation for many years, they do have significant disadvantages. This is foremost caused by the loss of word order within those representations. Also, words with similar meaning, such as "hate" and "loathe" are equally far apart within this representation as words with totally different meaning such as "hate" and "love". This means that this representation is less well equiped to deal with nuanced differences in meaning and context.\\
%Representing documents can also be done using word embeddings. Word embeddings are multi-dimensional vectors that represent the semantical meaning of words. These embeddings are created using a neighbourhood approach, and therefore words that appear in similar contexts also are located closeby within the multi-dimensional space. \cite{mikolov2013efficient} Documents can then be represented by ordering these word-vectors in the same sequence as original sentences.\\
%Using embedding spaces such as Word2Vec are thus not prone to the loss of word order. This means that with these representation also contextual nuances within sentences caused by word order can be understood. This is not possible using only words or bag-of-words, as it is unlikely that specific words occur in the same order within multiple documents.  Moreover, similar sentences are represented by closely related vectors, which allows systems to also understand sentences when other words are used. \\
%PAR2VEC
%
%\subsection{Classifiers}
%\subsubsection{Probalistic models}
%One of the most prominent ways for text classification is the Naive Bayes classifier. The Naive Bayes classifier learns the probability words occur within a certain topic. New documents can be classified using the distribution of words within that document. However, the Naive Bayes assumes that items occur independently of each other, and this assumption often does not hold. Still, the Naive Bayes can perform relatively well. \cite{allahyari2017brief} Two distinct ways of Naive Bayes can be found: Multi-variate Bernoulli model and multinomial model. The main difference is the input for the model. Within the multi-variate Bernoulli the binary vector representation is used and within the multinominal model the bag-of-words is used.\\
%For both models its learning process is relatively similar. For each word it is calculated how many times it occurs in each class. Then this number plus one is divided by the amount of unique words plus the amount of words within documents of that class. This function yields the probability of a document containing that word belongs to a certain class. To classify new documents the probabilities for the words within that document can be calculated. Using these probabilities the relative relative probability of the document to belong to a certain class can be calculated. \cite{mccallum1998comparison}\\
%Although Naive Bayes algorithms are performing quite well they do have some performance issues. It suffers from the problems of the binary and bag-of-words representation. Moreover, the independency assumption also prevents naive bayes from discovering more complex relations within texts, as it is unable to see that co-occurences of specific words can actually define the class of a sentence. \\ 
% 
%\subsubsection{Decision trees}
%Tree-based classifiers classify documents by creating a set of rules based on the presence and absence of words within documents. The training of decision trees is done by splitting the training set into multiple splits. Each individual split is based on the information gain provided, which means that by selecting on the specific feature of this split the greatest amount of extra documents can be classified correctly. To prevent overfitting the data is being split untill each node has a certain minimum of words or untill a maxinum number of splits are performed.
%
%\subsubsection{Linear models}
%\begin{itemize}
%\item SVM classifiers
%
%\item Regression-based classifiers
%
%\end{itemize}
%
%
%\subsubsection{Neural networks}





%Deze sectie bestaat uit een aantal "blokken", waarin je per blok de relevante literatuur beschrijft. 
%
%Neem alleen literatuur op die van belang is voor jouw onderzoeksvraag en deelvragen.
%
%Typisch heb je 1 blok voor je hoofdvraag en per deelvraag \textbf{RQi} een blok. 
%
%
%\subsection{RQ1}
%
%\subsection{RQ2}