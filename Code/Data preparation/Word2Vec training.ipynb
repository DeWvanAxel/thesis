{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC-Axel\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "import csv\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(article):\n",
    "    article = article.encode('utf-8')\n",
    "    article = str(article.lower())\n",
    "    return removeTagsInterpuntion(article)\n",
    "\n",
    "def removeTagsInterpuntion(article): #remove URL's, HTML-tags and interpuntion\n",
    "    article = re.sub(\"<\\w*>\", '', article)\n",
    "    article = re.sub(\"<\\w*\\s\\/>\", '', article)\n",
    "    article = re.sub(\"^https?:\\/\\/.*[\\r\\n]*\", '', article)\n",
    "    article = re.sub('\\\\\\\\x\\S.', '', article)\n",
    "    article = re.sub('[^a-z\\s]', '', article)\n",
    "    article = re.sub('\\d', '',article)\n",
    "    return article\n",
    "\n",
    "def json_readr(file):\n",
    "    for line in open(file, mode=\"r\"):\n",
    "        yield json.loads(line)\n",
    "\n",
    "def loadData(path, texts, labels, urls, highLevelLabels = True):\n",
    "    with open(path) as data_file:    \n",
    "        for line in data_file:\n",
    "            jsons = re.split('\\]\\[', line)\n",
    "            data = []\n",
    "            for newline in jsons:\n",
    "                newline = re.sub('\\]\\[', '\\[', newline)\n",
    "                if newline[0] is not \"[\":\n",
    "                    newline = \"[\" + newline\n",
    "                if newline[len(newline)-1] is not \"}\" and newline[len(newline)-1] is not \"]\":\n",
    "                    newline+= \"}\"\n",
    "                if newline[len(newline)-1] is not \"]\":\n",
    "                    newline+= \"]\"\n",
    "                betterload = json.loads(newline)\n",
    "                data.extend(betterload)\n",
    "    for article in data:\n",
    "        text = preprocess(article[\"content\"])\n",
    "        if len(article[\"category\"]) != 0:\n",
    "            if len(text.split()) > 9 and article[\"category\"][0] != \"NOCAT\": #Remove small sentences\n",
    "                if article[\"url\"] not in urls and \"kv-tk\" not in article[\"url\"]:\n",
    "                    urls.append(article[\"url\"])\n",
    "                    texts.append(word_tokenize(text))\n",
    "                    if highLevelLabels:\n",
    "                        label = []\n",
    "                        for lowLevelLabel in article[\"category\"]:\n",
    "                            if lowLevelLabel.split(\"|\")[0] not in label:\n",
    "                                label.append(lowLevelLabel.split(\"|\")[0])\n",
    "                        labels.append(label)\n",
    "                    else:\n",
    "                        labels.append(article[\"category\"])\n",
    "    return texts, labels, urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path done\n",
      "path done\n",
      "path done\n",
      "path done\n",
      "path done\n",
      "path done\n",
      "path done\n",
      "path done\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "labels = []\n",
    "urls = []\n",
    "paths = [r\"C:\\Users\\PC-Axel\\Documents\\github\\thesis\\Data\\PoliFLW Data\\kamerstukken_goed_jan2016.json\",\n",
    "        r\"C:\\Users\\PC-Axel\\Documents\\github\\thesis\\Data\\PoliFLW Data\\kamerstukken_goed_jan2015.json\",\n",
    "        r\"C:\\Users\\PC-Axel\\Documents\\github\\thesis\\Data\\PoliFLW Data\\kamerstukken_goed_okt2015.json\",\n",
    "        r\"C:\\Users\\PC-Axel\\Documents\\github\\thesis\\Data\\PoliFLW Data\\kamerstukken_goed_2017.json\",\n",
    "        r\"C:\\Users\\PC-Axel\\Documents\\github\\thesis\\Data\\PoliFLW Data\\kamerstukken_goed_2010-2014.json\",\n",
    "        r\"C:\\Users\\PC-Axel\\Documents\\github\\thesis\\Data\\PoliFLW Data\\kamerstukken_goed_2001-2003.json\",\n",
    "        r\"C:\\Users\\PC-Axel\\Documents\\github\\thesis\\Data\\PoliFLW Data\\kamerstukken_goed_2004-2006.json\",\n",
    "        r\"C:\\Users\\PC-Axel\\Documents\\github\\thesis\\Data\\PoliFLW Data\\kamerstukken_goed_2007-2009.json\"]\n",
    "\n",
    "for path in paths:\n",
    "    texts, labels, urls = loadData(path, texts, labels, urls)\n",
    "    print(\"path done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"C:\\Users\\PC-Axel\\Documents\\GitHub\\thesis\\Code\\Data preparation\\alleGemeentes.csv\", 'r') as csvfile:\n",
    "    next(csvfile)\n",
    "    spamreader = csv.reader(csvfile, delimiter='|')    \n",
    "    for row in spamreader:\n",
    "         texts.append(word_tokenize(preprocess(row[1])))\n",
    "            \n",
    "with open(r\"C:\\Users\\PC-Axel\\Documents\\GitHub\\thesis\\Code\\Data preparation\\alleGemeentes2.csv\", 'r') as csvfile:\n",
    "    next(csvfile)\n",
    "    spamreader = csv.reader(csvfile, delimiter='|')\n",
    "    for row in spamreader:\n",
    "         texts.append(word_tokenize(preprocess(row[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(texts, size=160, window=5, min_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors2 = KeyedVectors.load_word2vec_format(r\"C:\\Users\\PC-Axel\\Documents\\Codeer projecten\\Word2Vec Vectoren\\Nederlandse word2vec\\combined-160.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('buitenr', 0.8683865666389465), ('binnen', 0.7571752071380615), ('daarbuiten', 0.7403473854064941), ('elders', 0.7342738509178162), ('boven', 0.7157936692237854), ('vrij', 0.697746992111206), ('binnenr', 0.6930598020553589), ('beneden', 0.6908529996871948), ('omgekeerd', 0.6868816018104553), ('daaronder', 0.6829747557640076)]\n"
     ]
    }
   ],
   "source": [
    "print(word_vectors.most_similar_cosmul(positive=['buiten']))\n",
    "#print(word_vectors2.most_similar_cosmul(positive=['belasting']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors.save_word2vec_format(\"zelfgemaakte-w2v.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
