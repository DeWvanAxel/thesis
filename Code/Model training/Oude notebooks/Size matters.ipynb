{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC-Axel\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\PC-Axel\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\PC-Axel\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter,defaultdict\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import os\n",
    "\n",
    "#os.environ['KERAS_BACKEND']='theano'\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Merge, Dropout\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(article):\n",
    "    article = article.encode('utf-8')\n",
    "    article = str(article.lower())\n",
    "    return removeTagsInterpuntion(article)\n",
    "\n",
    "def removeTagsInterpuntion(article): #remove URL's, HTML-tags and interpuntion\n",
    "    article = re.sub(\"^https?:\\/\\/.*[\\r\\n]*\", '', article)\n",
    "    article = re.sub('\\\\\\\\x\\w\\w', '', article)\n",
    "    article = re.sub('[^a-z\\s]', '', article)\n",
    "    return article\n",
    "\n",
    "def loadData(path, texts, labels, urls, highLevelLabels = True):\n",
    "    with open(path) as data_file:    \n",
    "        data = json.load(data_file)\n",
    "    for article in data:\n",
    "        text = preprocess(article[\"content\"])\n",
    "        if len(text.split()) > 9 and article[\"category\"] != \"NOCAT\": #Remove small sentences\n",
    "            if article[\"url\"] not in urls:\n",
    "                urls.append(article[\"url\"])\n",
    "                texts.append(text)\n",
    "                if highLevelLabels:\n",
    "                    label = article[\"category\"].split(\"|\")\n",
    "                    labels.append(label[0])\n",
    "                else:\n",
    "                    labels.append(article[\"category\"])\n",
    "    return texts, labels, urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "labels = []\n",
    "urls = []\n",
    "paths = [r\"C:\\Users\\PC-Axel\\Documents\\github\\thesis\\Data\\PoliFLW Data\\kamerstukken_topics_first.json\",\n",
    "         r\"C:\\Users\\PC-Axel\\Documents\\github\\thesis\\Data\\PoliFLW Data\\kamerstukken_topics_second.json\",\n",
    "        r\"C:\\Users\\PC-Axel\\Documents\\github\\thesis\\Data\\PoliFLW Data\\kamerstukken_topics_2017.json\",\n",
    "        r\"C:\\Users\\PC-Axel\\Documents\\github\\thesis\\Data\\PoliFLW Data\\kamerstukken_topics_2016.json\"]\n",
    "\n",
    "for path in paths:\n",
    "    texts, labels, urls = loadData(path, texts, labels, urls)\n",
    "    \n",
    "\n",
    "#Prepare splitting\n",
    "VALIDATION_SPLIT = 0.2\n",
    "indices = np.arange(len(labels))\n",
    "np.random.shuffle(indices)\n",
    "texts = np.array(texts)[indices]\n",
    "labels = np.array(labels)[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * labels.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 161810 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 160\n",
    "\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "labelsCNN = lb.fit_transform(labels)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 1442951 word vectors in Glove 6B 100d.\n"
     ]
    }
   ],
   "source": [
    "GLOVE_DIR = r\"C:\\Users\\PC-Axel\\Documents\\Codeer projecten\\Word2Vec Vectoren\\Nederlandse word2vec\\combined-160.txt\"\n",
    "embeddings_index = {}\n",
    "f = open(GLOVE_DIR, encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Total %s word vectors in Glove 6B 100d.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\PC-Axel\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "l_cov1= Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "l_pool1 = MaxPooling1D(5)(l_cov1)\n",
    "l_cov2 = Conv1D(128, 5, activation='relu')(l_pool1)\n",
    "l_pool2 = MaxPooling1D(5)(l_cov2)\n",
    "l_cov3 = Conv1D(128, 5, activation='relu')(l_pool2)\n",
    "l_pool3 = MaxPooling1D(35)(l_cov3)  # global max pooling\n",
    "l_flat = Flatten()(l_pool3)\n",
    "l_dense = Dense(128, activation='relu')(l_flat)\n",
    "preds = Dense(labelsCNN.shape[1], activation='softmax')(l_dense)\n",
    "\n",
    "modelCNN = Model(sequence_input, preds)\n",
    "modelCNN.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train on 2327 samples, validate on 582 samples\n",
      "Epoch 1/20\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 2.7203 - acc: 0.1521 - val_loss: 2.7050 - val_acc: 0.1718\n",
      "Epoch 2/20\n",
      "2327/2327 [==============================] - 28s 12ms/step - loss: 2.6220 - acc: 0.1835 - val_loss: 2.6115 - val_acc: 0.2251\n",
      "Epoch 3/20\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 2.4516 - acc: 0.2432 - val_loss: 2.4196 - val_acc: 0.2354\n",
      "Epoch 4/20\n",
      "2327/2327 [==============================] - 26s 11ms/step - loss: 2.2562 - acc: 0.3167 - val_loss: 2.3404 - val_acc: 0.2938\n",
      "Epoch 5/20\n",
      "2327/2327 [==============================] - 27s 12ms/step - loss: 2.0810 - acc: 0.3782 - val_loss: 2.2622 - val_acc: 0.2801\n",
      "Epoch 6/20\n",
      "2327/2327 [==============================] - 28s 12ms/step - loss: 1.9146 - acc: 0.4074 - val_loss: 1.9893 - val_acc: 0.4038\n",
      "Epoch 7/20\n",
      "2327/2327 [==============================] - 26s 11ms/step - loss: 1.7430 - acc: 0.4688 - val_loss: 2.1375 - val_acc: 0.2869\n",
      "Epoch 8/20\n",
      "2327/2327 [==============================] - 26s 11ms/step - loss: 1.5824 - acc: 0.4826 - val_loss: 1.9468 - val_acc: 0.3900\n",
      "Epoch 9/20\n",
      "2327/2327 [==============================] - 26s 11ms/step - loss: 1.4461 - acc: 0.5385 - val_loss: 1.8279 - val_acc: 0.4089\n",
      "Epoch 10/20\n",
      "2327/2327 [==============================] - 26s 11ms/step - loss: 1.2824 - acc: 0.5900 - val_loss: 2.6251 - val_acc: 0.2663\n",
      "Epoch 11/20\n",
      "2327/2327 [==============================] - 26s 11ms/step - loss: 1.1980 - acc: 0.6119 - val_loss: 2.3172 - val_acc: 0.3694\n",
      "Epoch 12/20\n",
      "2327/2327 [==============================] - 28s 12ms/step - loss: 1.0566 - acc: 0.6708 - val_loss: 1.8135 - val_acc: 0.4777\n",
      "Epoch 13/20\n",
      "2327/2327 [==============================] - 26s 11ms/step - loss: 0.9047 - acc: 0.7086 - val_loss: 2.0672 - val_acc: 0.4003\n",
      "Epoch 14/20\n",
      "2327/2327 [==============================] - 28s 12ms/step - loss: 0.7341 - acc: 0.7709 - val_loss: 1.8206 - val_acc: 0.4742\n",
      "Epoch 15/20\n",
      "2327/2327 [==============================] - 28s 12ms/step - loss: 0.6773 - acc: 0.7843 - val_loss: 1.8111 - val_acc: 0.5155\n",
      "Epoch 16/20\n",
      "2327/2327 [==============================] - 26s 11ms/step - loss: 0.5533 - acc: 0.8281 - val_loss: 1.9780 - val_acc: 0.5120\n",
      "Epoch 17/20\n",
      "2327/2327 [==============================] - 26s 11ms/step - loss: 0.4326 - acc: 0.8715 - val_loss: 1.7992 - val_acc: 0.5533\n",
      "Epoch 18/20\n",
      "2327/2327 [==============================] - 26s 11ms/step - loss: 0.4176 - acc: 0.8771 - val_loss: 2.4714 - val_acc: 0.4227\n",
      "Epoch 19/20\n",
      "2327/2327 [==============================] - 26s 11ms/step - loss: 0.4374 - acc: 0.8861 - val_loss: 1.9131 - val_acc: 0.5825\n",
      "Epoch 20/20\n",
      "2327/2327 [==============================] - 26s 11ms/step - loss: 0.2162 - acc: 0.9368 - val_loss: 2.3771 - val_acc: 0.4759\n",
      "\n",
      "Train on 4653 samples, validate on 1163 samples\n",
      "Epoch 1/20\n",
      "4653/4653 [==============================] - 52s 11ms/step - loss: 1.6172 - acc: 0.5749 - val_loss: 1.4638 - val_acc: 0.5572\n",
      "Epoch 2/20\n",
      "4653/4653 [==============================] - 51s 11ms/step - loss: 1.1822 - acc: 0.6548 - val_loss: 1.3094 - val_acc: 0.6139\n",
      "Epoch 3/20\n",
      "4653/4653 [==============================] - 52s 11ms/step - loss: 0.9496 - acc: 0.7176 - val_loss: 1.3546 - val_acc: 0.6036\n",
      "Epoch 4/20\n",
      "4653/4653 [==============================] - 51s 11ms/step - loss: 0.7672 - acc: 0.7722 - val_loss: 1.3365 - val_acc: 0.6156\n",
      "Epoch 5/20\n",
      "4653/4653 [==============================] - 54s 12ms/step - loss: 0.5730 - acc: 0.8302 - val_loss: 1.5137 - val_acc: 0.5993\n",
      "Epoch 6/20\n",
      "4653/4653 [==============================] - 54s 12ms/step - loss: 0.4327 - acc: 0.8719 - val_loss: 1.8509 - val_acc: 0.5460\n",
      "Epoch 7/20\n",
      "4653/4653 [==============================] - 52s 11ms/step - loss: 0.3142 - acc: 0.9100 - val_loss: 1.5564 - val_acc: 0.6277\n",
      "Epoch 8/20\n",
      "4653/4653 [==============================] - 51s 11ms/step - loss: 0.2443 - acc: 0.9323 - val_loss: 1.9277 - val_acc: 0.6113\n",
      "Epoch 9/20\n",
      "4653/4653 [==============================] - 51s 11ms/step - loss: 0.1620 - acc: 0.9577 - val_loss: 1.6820 - val_acc: 0.6518\n",
      "Epoch 10/20\n",
      "4653/4653 [==============================] - 59s 13ms/step - loss: 0.1351 - acc: 0.9654 - val_loss: 1.9538 - val_acc: 0.6062\n",
      "Epoch 11/20\n",
      "4653/4653 [==============================] - 52s 11ms/step - loss: 0.1277 - acc: 0.9675 - val_loss: 1.8841 - val_acc: 0.6561\n",
      "Epoch 12/20\n",
      "4653/4653 [==============================] - 51s 11ms/step - loss: 0.1586 - acc: 0.9673 - val_loss: 1.9378 - val_acc: 0.6586\n",
      "Epoch 13/20\n",
      "4653/4653 [==============================] - 51s 11ms/step - loss: 0.0185 - acc: 0.9976 - val_loss: 3.8040 - val_acc: 0.5159\n",
      "Epoch 14/20\n",
      "4653/4653 [==============================] - 51s 11ms/step - loss: 0.1113 - acc: 0.9776 - val_loss: 2.2305 - val_acc: 0.6561\n",
      "Epoch 15/20\n",
      "4653/4653 [==============================] - 51s 11ms/step - loss: 0.0910 - acc: 0.9813 - val_loss: 2.2571 - val_acc: 0.6604\n",
      "Epoch 16/20\n",
      "4653/4653 [==============================] - 51s 11ms/step - loss: 0.0230 - acc: 0.9927 - val_loss: 2.3780 - val_acc: 0.6543\n",
      "Epoch 17/20\n",
      "4653/4653 [==============================] - 51s 11ms/step - loss: 0.0476 - acc: 0.9897 - val_loss: 3.3880 - val_acc: 0.5795\n",
      "Epoch 18/20\n",
      "4653/4653 [==============================] - 51s 11ms/step - loss: 0.0167 - acc: 0.9957 - val_loss: 2.5402 - val_acc: 0.6664\n",
      "Epoch 19/20\n",
      "4653/4653 [==============================] - 51s 11ms/step - loss: 0.0561 - acc: 0.9847 - val_loss: 2.4649 - val_acc: 0.6724\n",
      "Epoch 20/20\n",
      "4653/4653 [==============================] - 51s 11ms/step - loss: 0.0052 - acc: 0.9989 - val_loss: 3.1949 - val_acc: 0.5606\n",
      "\n",
      "Train on 6980 samples, validate on 1745 samples\n",
      "Epoch 1/20\n",
      "6980/6980 [==============================] - 77s 11ms/step - loss: 1.3114 - acc: 0.7029 - val_loss: 1.0550 - val_acc: 0.7089\n",
      "Epoch 2/20\n",
      "6980/6980 [==============================] - 77s 11ms/step - loss: 0.7671 - acc: 0.7834 - val_loss: 1.0474 - val_acc: 0.7032\n",
      "Epoch 3/20\n",
      "6980/6980 [==============================] - 77s 11ms/step - loss: 0.5441 - acc: 0.8450 - val_loss: 1.2114 - val_acc: 0.6779\n",
      "Epoch 4/20\n",
      "6980/6980 [==============================] - 77s 11ms/step - loss: 0.3512 - acc: 0.8997 - val_loss: 1.2515 - val_acc: 0.7072\n",
      "Epoch 5/20\n",
      "6980/6980 [==============================] - 77s 11ms/step - loss: 0.2238 - acc: 0.9319 - val_loss: 1.2961 - val_acc: 0.7152\n",
      "Epoch 6/20\n",
      "6980/6980 [==============================] - 78s 11ms/step - loss: 0.1346 - acc: 0.9628 - val_loss: 1.4726 - val_acc: 0.7020\n",
      "Epoch 7/20\n",
      "6980/6980 [==============================] - 80s 11ms/step - loss: 0.0805 - acc: 0.9771 - val_loss: 2.4103 - val_acc: 0.6029\n",
      "Epoch 8/20\n",
      "6980/6980 [==============================] - 79s 11ms/step - loss: 0.0739 - acc: 0.9808 - val_loss: 1.7283 - val_acc: 0.7140\n",
      "Epoch 9/20\n",
      "6980/6980 [==============================] - 79s 11ms/step - loss: 0.0368 - acc: 0.9904 - val_loss: 1.8862 - val_acc: 0.7249\n",
      "Epoch 10/20\n",
      "6980/6980 [==============================] - 77s 11ms/step - loss: 0.0724 - acc: 0.9819 - val_loss: 1.9717 - val_acc: 0.7129\n",
      "Epoch 11/20\n",
      "6980/6980 [==============================] - 77s 11ms/step - loss: 0.0064 - acc: 0.9990 - val_loss: 2.6512 - val_acc: 0.6464\n",
      "Epoch 12/20\n",
      "6980/6980 [==============================] - 77s 11ms/step - loss: 0.0569 - acc: 0.9881 - val_loss: 2.7640 - val_acc: 0.6504\n",
      "Epoch 13/20\n",
      "6980/6980 [==============================] - 77s 11ms/step - loss: 0.0065 - acc: 0.9990 - val_loss: 2.2104 - val_acc: 0.7192\n",
      "Epoch 14/20\n",
      "6980/6980 [==============================] - 77s 11ms/step - loss: 0.0385 - acc: 0.9928 - val_loss: 2.2349 - val_acc: 0.7192\n",
      "Epoch 15/20\n",
      "6980/6980 [==============================] - 77s 11ms/step - loss: 0.0320 - acc: 0.9941 - val_loss: 2.3823 - val_acc: 0.7077\n",
      "Epoch 16/20\n",
      "6980/6980 [==============================] - 77s 11ms/step - loss: 0.0031 - acc: 0.9997 - val_loss: 2.3631 - val_acc: 0.7266\n",
      "Epoch 17/20\n",
      "6980/6980 [==============================] - 77s 11ms/step - loss: 0.0421 - acc: 0.9928 - val_loss: 2.7097 - val_acc: 0.6974\n",
      "Epoch 18/20\n",
      "6980/6980 [==============================] - 77s 11ms/step - loss: 0.0033 - acc: 0.9997 - val_loss: 2.5040 - val_acc: 0.7255\n",
      "Epoch 19/20\n",
      "6980/6980 [==============================] - 77s 11ms/step - loss: 0.0023 - acc: 0.9999 - val_loss: 2.5697 - val_acc: 0.7238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20\n",
      "6980/6980 [==============================] - 77s 11ms/step - loss: 0.0370 - acc: 0.9956 - val_loss: 2.6749 - val_acc: 0.7192\n",
      "\n",
      "Train on 9306 samples, validate on 2326 samples\n",
      "Epoch 1/20\n",
      "9306/9306 [==============================] - 103s 11ms/step - loss: 1.2179 - acc: 0.7657 - val_loss: 0.7967 - val_acc: 0.7997\n",
      "Epoch 2/20\n",
      "9306/9306 [==============================] - 102s 11ms/step - loss: 0.6043 - acc: 0.8500 - val_loss: 0.8368 - val_acc: 0.7872\n",
      "Epoch 3/20\n",
      "9306/9306 [==============================] - 103s 11ms/step - loss: 0.3608 - acc: 0.9075 - val_loss: 0.8936 - val_acc: 0.7790\n",
      "Epoch 4/20\n",
      "9306/9306 [==============================] - 102s 11ms/step - loss: 0.2135 - acc: 0.9415 - val_loss: 0.9358 - val_acc: 0.7954\n",
      "Epoch 5/20\n",
      "9306/9306 [==============================] - 103s 11ms/step - loss: 0.1198 - acc: 0.9677 - val_loss: 0.9389 - val_acc: 0.7979\n",
      "Epoch 6/20\n",
      "9306/9306 [==============================] - 102s 11ms/step - loss: 0.0783 - acc: 0.9798 - val_loss: 1.0719 - val_acc: 0.7945\n",
      "Epoch 7/20\n",
      "9306/9306 [==============================] - 102s 11ms/step - loss: 0.0473 - acc: 0.9882 - val_loss: 1.1247 - val_acc: 0.7997\n",
      "Epoch 8/20\n",
      "9306/9306 [==============================] - 103s 11ms/step - loss: 0.0385 - acc: 0.9897 - val_loss: 1.2459 - val_acc: 0.7958\n",
      "Epoch 9/20\n",
      "9306/9306 [==============================] - 102s 11ms/step - loss: 0.0302 - acc: 0.9951 - val_loss: 1.3147 - val_acc: 0.8040\n",
      "Epoch 10/20\n",
      "9306/9306 [==============================] - 103s 11ms/step - loss: 0.0470 - acc: 0.9905 - val_loss: 1.3501 - val_acc: 0.7876\n",
      "Epoch 11/20\n",
      "9306/9306 [==============================] - 102s 11ms/step - loss: 0.0177 - acc: 0.9977 - val_loss: 1.4698 - val_acc: 0.7872\n",
      "Epoch 12/20\n",
      "9306/9306 [==============================] - 102s 11ms/step - loss: 0.0279 - acc: 0.9953 - val_loss: 1.6365 - val_acc: 0.7816\n",
      "Epoch 13/20\n",
      "9306/9306 [==============================] - 103s 11ms/step - loss: 0.0097 - acc: 0.9994 - val_loss: 1.6109 - val_acc: 0.7979\n",
      "Epoch 14/20\n",
      "9306/9306 [==============================] - 103s 11ms/step - loss: 0.0329 - acc: 0.9956 - val_loss: 1.6104 - val_acc: 0.7936\n",
      "Epoch 15/20\n",
      "9306/9306 [==============================] - 103s 11ms/step - loss: 0.0261 - acc: 0.9967 - val_loss: 1.7661 - val_acc: 0.7936\n",
      "Epoch 16/20\n",
      "9306/9306 [==============================] - 102s 11ms/step - loss: 0.0260 - acc: 0.9963 - val_loss: 1.6926 - val_acc: 0.7919\n",
      "Epoch 17/20\n",
      "9306/9306 [==============================] - 103s 11ms/step - loss: 0.0260 - acc: 0.9956 - val_loss: 1.7514 - val_acc: 0.7863\n",
      "Epoch 18/20\n",
      "9306/9306 [==============================] - 103s 11ms/step - loss: 0.0109 - acc: 0.9989 - val_loss: 1.7091 - val_acc: 0.7915\n",
      "Epoch 19/20\n",
      "9306/9306 [==============================] - 102s 11ms/step - loss: 0.0210 - acc: 0.9963 - val_loss: 1.7034 - val_acc: 0.7876\n",
      "Epoch 20/20\n",
      "9306/9306 [==============================] - 102s 11ms/step - loss: 0.0227 - acc: 0.9967 - val_loss: 1.7328 - val_acc: 0.7915\n",
      "\n",
      "Train on 11633 samples, validate on 2908 samples\n",
      "Epoch 1/20\n",
      "11633/11633 [==============================] - 129s 11ms/step - loss: 0.7305 - acc: 0.8331 - val_loss: 0.6179 - val_acc: 0.8415\n",
      "Epoch 2/20\n",
      "11633/11633 [==============================] - 128s 11ms/step - loss: 0.3581 - acc: 0.9068 - val_loss: 0.6004 - val_acc: 0.8569\n",
      "Epoch 3/20\n",
      "11633/11633 [==============================] - 128s 11ms/step - loss: 0.1823 - acc: 0.9506 - val_loss: 0.6397 - val_acc: 0.8604\n",
      "Epoch 4/20\n",
      "11633/11633 [==============================] - 131s 11ms/step - loss: 0.0945 - acc: 0.9761 - val_loss: 0.7159 - val_acc: 0.8511\n",
      "Epoch 5/20\n",
      "11633/11633 [==============================] - 132s 11ms/step - loss: 0.0583 - acc: 0.9853 - val_loss: 0.7655 - val_acc: 0.8573\n",
      "Epoch 6/20\n",
      "11633/11633 [==============================] - 129s 11ms/step - loss: 0.0497 - acc: 0.9896 - val_loss: 0.8190 - val_acc: 0.8618\n",
      "Epoch 7/20\n",
      "11633/11633 [==============================] - 128s 11ms/step - loss: 0.0354 - acc: 0.9934 - val_loss: 0.9040 - val_acc: 0.8556\n",
      "Epoch 8/20\n",
      "11633/11633 [==============================] - 128s 11ms/step - loss: 0.0295 - acc: 0.9954 - val_loss: 0.8866 - val_acc: 0.8545\n",
      "Epoch 9/20\n",
      "11633/11633 [==============================] - 128s 11ms/step - loss: 0.0230 - acc: 0.9961 - val_loss: 0.9673 - val_acc: 0.8535\n",
      "Epoch 10/20\n",
      "11633/11633 [==============================] - 128s 11ms/step - loss: 0.0195 - acc: 0.9967 - val_loss: 1.0085 - val_acc: 0.8528\n",
      "Epoch 11/20\n",
      "11633/11633 [==============================] - 128s 11ms/step - loss: 0.0257 - acc: 0.9957 - val_loss: 1.4371 - val_acc: 0.8064\n",
      "Epoch 12/20\n",
      "11633/11633 [==============================] - 128s 11ms/step - loss: 0.0192 - acc: 0.9981 - val_loss: 1.1154 - val_acc: 0.8339\n",
      "Epoch 13/20\n",
      "11633/11633 [==============================] - 128s 11ms/step - loss: 0.0317 - acc: 0.9960 - val_loss: 1.1023 - val_acc: 0.8511\n",
      "Epoch 14/20\n",
      "11633/11633 [==============================] - 128s 11ms/step - loss: 0.0196 - acc: 0.9972 - val_loss: 1.1188 - val_acc: 0.8518\n",
      "Epoch 15/20\n",
      "11633/11633 [==============================] - 128s 11ms/step - loss: 0.0186 - acc: 0.9980 - val_loss: 1.1922 - val_acc: 0.8480\n",
      "Epoch 16/20\n",
      "11633/11633 [==============================] - 128s 11ms/step - loss: 0.0192 - acc: 0.9971 - val_loss: 1.3140 - val_acc: 0.8312\n",
      "Epoch 17/20\n",
      "11633/11633 [==============================] - 22239s 2s/step - loss: 0.0151 - acc: 0.9981 - val_loss: 1.2671 - val_acc: 0.8377\n",
      "Epoch 18/20\n",
      "11633/11633 [==============================] - 135s 12ms/step - loss: 0.0231 - acc: 0.9963 - val_loss: 1.3123 - val_acc: 0.8346\n",
      "Epoch 19/20\n",
      "11633/11633 [==============================] - 130s 11ms/step - loss: 0.0220 - acc: 0.9969 - val_loss: 1.2928 - val_acc: 0.8439\n",
      "Epoch 20/20\n",
      "11633/11633 [==============================] - 130s 11ms/step - loss: 0.0230 - acc: 0.9966 - val_loss: 1.2830 - val_acc: 0.8387\n",
      "\n",
      "Train on 13960 samples, validate on 3490 samples\n",
      "Epoch 1/20\n",
      "13960/13960 [==============================] - 164s 12ms/step - loss: 0.5318 - acc: 0.8797 - val_loss: 0.5230 - val_acc: 0.8791\n",
      "Epoch 2/20\n",
      "13960/13960 [==============================] - 170s 12ms/step - loss: 0.2345 - acc: 0.9441 - val_loss: 0.5264 - val_acc: 0.8814\n",
      "Epoch 3/20\n",
      "13960/13960 [==============================] - 169s 12ms/step - loss: 0.1195 - acc: 0.9708 - val_loss: 0.5094 - val_acc: 0.8940\n",
      "Epoch 4/20\n",
      "13960/13960 [==============================] - 168s 12ms/step - loss: 0.0682 - acc: 0.9854 - val_loss: 0.6956 - val_acc: 0.8819\n",
      "Epoch 5/20\n",
      "12288/13960 [=========================>....] - ETA: 18s - loss: 0.0502 - acc: 0.9907"
     ]
    }
   ],
   "source": [
    "for percentage in [.1,.2,.3,.4,.5,.6,.7,.8,.9,1]:\n",
    "    np.random.shuffle(indices)\n",
    "    data = np.array(data)[indices]\n",
    "    labelsCNN = np.array(labelsCNN)[indices]\n",
    "    x_train = data[:-nb_validation_samples]\n",
    "    y_train = labelsCNN[:-nb_validation_samples]\n",
    "    x_val = data[-nb_validation_samples:]\n",
    "    y_val = labelsCNN[-nb_validation_samples:]\n",
    "    \n",
    "    print()\n",
    "    x_train = x_train[:round(percentage*x_train.shape[0])]\n",
    "    y_train = y_train[:round(percentage*y_train.shape[0])]\n",
    "    x_val = x_val[:round(percentage*x_val.shape[0])]\n",
    "    y_val = y_val[:round(percentage*y_val.shape[0])]\n",
    "    \n",
    "    modelCNN = Model(sequence_input, preds)\n",
    "    modelCNN.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['acc'])\n",
    "      \n",
    "    historyCNN = modelCNN.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          epochs=20, batch_size=128)#,callbacks=[metrics])\n",
    "    \n",
    "    modelCNN.save('models/CNN-Model-17-' + str(percentage).replace(\".\",\"\"))  # creates a HDF5 file 'my_model.h5'\n",
    "    with open('models/CNN-Model-17-history-' + str(percentage).replace(\".\",\"\"), 'wb') as file_pi:\n",
    "        pickle.dump(historyCNN.history, file_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.804887858592854, 2.6581932932817223, 2.659051361747909, 2.639732904479285, 2.626328833686163, 2.607622105623449, 2.5767467257594365, 2.530161949509349, 2.476153380685371, 2.3935008974202363, 2.3110649102124525, 2.185770446566691, 2.018958732709897, 1.8451103406962683, 1.662295182426446, 1.439953331226026, 1.2460666195128063, 1.006705300255707, 0.8300762046648803, 0.6590682172601091]\n",
      "0.04295532660129963\n",
      "0.12037833195150872\n",
      "0.19369627478129542\n"
     ]
    }
   ],
   "source": [
    "print(histories[0].history[\"loss\"])\n",
    "losses = []\n",
    "for history in histories:\n",
    "    print(min(history.history[\"val_acc\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
