{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter,defaultdict\n",
    "import csv\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import os\n",
    "import ast\n",
    "from gensim.models.doc2vec import Doc2Vec,LabeledSentence\n",
    "\n",
    "#os.environ['KERAS_BACKEND']='theano'\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Merge, Dropout\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import Callback\n",
    "import keras.objectives\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score, classification_report,jaccard_similarity_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "import numpy as np\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "from sklearn.base import clone\n",
    "from sklearn.multiclass import _ConstantPredictor, OneVsRestClassifier\n",
    "from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "labels = []\n",
    "urls = []\n",
    "\n",
    "with open('parlementData_2001-2017.csv', 'r') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter='&',quotechar='|')\n",
    "    for row in spamreader:\n",
    "        urls.append(row[0])\n",
    "        texts.append(row[1])\n",
    "        labels.append(ast.literal_eval(row[2]))\n",
    "        \n",
    "munTexts = []\n",
    "munLabels = []\n",
    "munNumber = []\n",
    "\n",
    "with open('municipality.csv', 'r') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter='&',quotechar='|')\n",
    "    for row in spamreader:\n",
    "        munNumber.append(row[0])\n",
    "        munTexts.append(row[1])\n",
    "        munLabels.append(ast.literal_eval(row[2]))\n",
    "        \n",
    "with open('municipalityFam.csv', 'r') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter='&',quotechar='|')\n",
    "    for row in spamreader:\n",
    "        munNumber.append(row[0])\n",
    "        munTexts.append(row[1])\n",
    "        munLabels.append(ast.literal_eval(row[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "439\n"
     ]
    }
   ],
   "source": [
    "print(len(munLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 392519 unique tokens.\n",
      "Shape of data tensor: (52397, 1000)\n",
      "Shape of label tensor: (52397, 17)\n",
      "Class distribution in training and validation set \n",
      "[2916 3559 6695 3454 1079 6047 1815 3192 4703 2935 8692 8222 1637 1591\n",
      " 3651 2902 9145]\n",
      "[ 523  632 1206  591  181 1070  330  513  873  574 1557 1437  257  294\n",
      "  698  519 1595]\n"
     ]
    }
   ],
   "source": [
    "TEST_SET = 0.15\n",
    "nb_test_samples = int(TEST_SET * len(labels))\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 160\n",
    "\n",
    "lb = MultiLabelBinarizer()\n",
    "labelsCNN = lb.fit_transform(labels)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labelsCNN.shape)\n",
    "\n",
    "x_train = data[:-nb_test_samples]\n",
    "y_train = labelsCNN[:-nb_test_samples]\n",
    "x_test = data[-nb_test_samples:]\n",
    "y_test = labelsCNN[-nb_test_samples:]\n",
    "\n",
    "xTrain = texts[:-nb_test_samples]\n",
    "xTest = texts[-nb_test_samples:]\n",
    "\n",
    "#transformer = TfidfVectorizer(smooth_idf=False, min_df=0.00001, max_df=0.2, sublinear_tf=True)\n",
    "    \n",
    "#xTrain = transformer.fit_transform(xTrain)\n",
    "#xTest = transformer.transform(xTest)\n",
    "\n",
    "print('Class distribution in training and validation set ')\n",
    "print(y_train.sum(axis=0))\n",
    "print(y_test.sum(axis=0))\n",
    "\n",
    "#xMun = transformer.transform(munTexts)\n",
    "#yMun = lb.transform(munLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(yVal,yPred):\n",
    "    print(\"F1 is \"        + str(round(f1_score(yVal, yPred, average=\"micro\"),3)) + \n",
    "        \", Precision is \" + str(round(precision_score(yVal, yPred, average=\"micro\"),3)) +\n",
    "        \", Recall is \"    + str(round(recall_score(yVal, yPred, average=\"micro\"),3)) +\n",
    "        \", Accuracy is \"  + str(round(accuracy_score(yVal, yPred),3)))\n",
    "\n",
    "\n",
    "def evaluationWithBestThreshold(clf, xMun, yMun, Thresholds = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]):\n",
    "    bestF1 = 0\n",
    "    bestThreshold = 0\n",
    "    yPred = clf.predict_proba(xMun)\n",
    "    for threshold in [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]:\n",
    "        F1 = f1_score(yMun, (yPred > threshold).astype(int), average=\"micro\")\n",
    "        if F1 > bestF1: \n",
    "            bestF1 = F1\n",
    "            bestThreshold = threshold\n",
    "    evaluation(yMun,(yPred > bestThreshold).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(article):\n",
    "    article = article.encode('utf-8')\n",
    "    article = str(article.lower())\n",
    "    return removeTagsInterpuntion(article)\n",
    "\n",
    "def removeTagsInterpuntion(article): #remove URL's, HTML-tags and interpuntion\n",
    "    article = re.sub(\"<\\w*>\", '', article)\n",
    "    article = re.sub(\"<\\w*\\s\\/>\", '', article)\n",
    "    article = re.sub(\"^https?:\\/\\/.*[\\r\\n]*\", '', article)\n",
    "    article = re.sub('\\\\\\\\x\\S.', '', article)\n",
    "    article = re.sub('[^a-z\\s]', '', article)\n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "municipality = []\n",
    "\n",
    "with open(r\"C:\\Users\\PC-Axel\\Documents\\GitHub\\thesis\\Code\\Data preparation\\alleGemeentes2.csv\", 'r') as csvfile:\n",
    "    next(csvfile)\n",
    "    spamreader = csv.reader(csvfile, delimiter='|')\n",
    "    \n",
    "    for row in spamreader:\n",
    "        municipality.append(preprocess(row[1]))\n",
    "\n",
    "indices = np.arange(len(municipality))\n",
    "np.random.shuffle(indices)\n",
    "municipality = np.array(municipality)[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "label = [0] * len(xTrain[:10000])\n",
    "label.extend([1] * len(municipality[:10000]))\n",
    "print(len(label))\n",
    "allData = [] \n",
    "allData.extend(xTrain[:10000])\n",
    "allData.extend(municipality[:10000])\n",
    "print(len(allData))\n",
    "\n",
    "indices = np.arange(len(label))\n",
    "np.random.shuffle(indices)\n",
    "allData = np.array(allData)[indices]\n",
    "label = np.array(label)[indices]\n",
    "\n",
    "xTrainWeigth = allData[:-1000]\n",
    "yTrainWeigth = label[:-1000]\n",
    "xTestWeigth = allData[-1000:]\n",
    "yTestWeigth = label[-1000:]\n",
    "\n",
    "transformer = TfidfVectorizer(smooth_idf=False, min_df=0.00001, max_df=0.2, sublinear_tf=True)\n",
    "    \n",
    "xTrainWeigth = transformer.fit_transform(xTrainWeigth)\n",
    "xTestWeigth = transformer.transform(xTestWeigth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 is 0.998, Precision is 0.998, Recall is 0.998, Accuracy is 0.998\n"
     ]
    }
   ],
   "source": [
    "clf =LogisticRegression(dual=False)\n",
    "clf.fit(xTrainWeigth, yTrainWeigth)\n",
    "yPred = clf.predict(xTestWeigth)\n",
    "evaluation(yTestWeigth,yPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weights = transformer.transform(xTrain[1000:])\n",
    "weights = (1.-np.array(clf.predict_proba(transformer.transform(xTrain[10000:])))[:,0])/ np.array(clf.predict_proba(transformer.transform(xTrain[10000:])))[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44538, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34538\n"
     ]
    }
   ],
   "source": [
    "print(len(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "newTransformer = TfidfVectorizer(smooth_idf=False, min_df=0.00001, max_df=0.2, sublinear_tf=True)\n",
    "xTrain = newTransformer.fit_transform(xTrain[10000:])\n",
    "xTest = newTransformer.transform(xTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fit_binary_new(estimator, X, y, sample_weight, classes=None):\n",
    "    unique_y = np.unique(y)\n",
    "    if len(unique_y) == 1:\n",
    "        if classes is not None:\n",
    "            if y[0] == -1:\n",
    "                c = 0\n",
    "            else:\n",
    "                c = y[0]\n",
    "            warnings.warn(\"Label %s is present in all training examples.\" %\n",
    "                          str(classes[c]))\n",
    "        estimator = _ConstantPredictor().fit(X, unique_y)\n",
    "    else:\n",
    "        estimator = clone(estimator)\n",
    "\n",
    "        # Only this changed\n",
    "        estimator.fit(X, y, sample_weight=sample_weight)\n",
    "    return estimator\n",
    "\n",
    "class OneVsRestClassifierNew(OneVsRestClassifier):\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        self.label_binarizer_ = LabelBinarizer(sparse_output=True)\n",
    "        Y = self.label_binarizer_.fit_transform(y)\n",
    "        Y = Y.tocsc()\n",
    "        self.classes_ = self.label_binarizer_.classes_\n",
    "        columns = (col.toarray().ravel() for col in Y.T)\n",
    "        self.estimators_ = Parallel(n_jobs=self.n_jobs)(delayed(_fit_binary_new)(\n",
    "            self.estimator, X, column, sample_weight, classes=[\n",
    "                \"not %s\" % self.label_binarizer_.classes_[i],\n",
    "                self.label_binarizer_.classes_[i]])\n",
    "            for i, column in enumerate(columns))\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifierNew(estimator=SGDClassifier(alpha=1e-05, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='modified_huber', max_iter=10,\n",
       "       n_iter=None, n_jobs=1, penalty='l2', power_t=0.5, random_state=42,\n",
       "       shuffle=True, tol=None, verbose=0, warm_start=False),\n",
       "            n_jobs=1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y_mlb = MultiLabelBinarizer().fit_transform(y_train)\n",
    "clf = OneVsRestClassifierNew(SGDClassifier(alpha = 1e-05, loss = 'modified_huber', penalty = 'l2', random_state=42, max_iter=10, tol=None))\n",
    "clf.fit(xTrain, y_train[10000:], sample_weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 is 0.729, Precision is 0.697, Recall is 0.763, Accuracy is 0.401\n"
     ]
    }
   ],
   "source": [
    "#y_test = MultiLabelBinarizer().fit_transform(y_test)\n",
    "prediction = clf.predict(xTest)\n",
    "evaluationWithBestThreshold(clf, xTest, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "xMun = newTransformer.transform(munTexts)\n",
    "yMun = lb.transform(munLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 is 0.363, Precision is 0.352, Recall is 0.374, Accuracy is 0.132\n"
     ]
    }
   ],
   "source": [
    "#prediction = clf.predict(xMun)\n",
    "evaluationWithBestThreshold(clf, xMun, yMun, Thresholds = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9])\n",
    "#evaluation(yMun,prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34538\n",
      "(34538, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(len(weights))\n",
    "print(x_train[10000:].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def fmeasure(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "    p = true_positives / (predicted_positives + K.epsilon())\n",
    "    r = true_positives / (possible_positives + K.epsilon())\n",
    "\n",
    "    beta = 1 # fmeasure\n",
    "    bb = beta**2\n",
    "    fbeta_score = (1 + bb) * (p * r) / (bb * p + r + K.epsilon())\n",
    "    return fbeta_score\n",
    "\n",
    "import keras.objectives\n",
    "keras.objectives.fmeasure = fmeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_DIRS   = [r\"C:\\Users\\PC-Axel\\Documents\\Codeer projecten\\Word2Vec Vectoren\\Nederlandse word2vec\\combined-160.txt\",\n",
    "                r\"C:\\Users\\PC-Axel\\Documents\\GitHub\\thesis\\Code\\Data preparation\\zelfgemaakte-w2v.txt\"]\n",
    "TRAINABLE    = [True, False]\n",
    "SPLIT        = [True, False]\n",
    "MULTIFILTER  = [True, False]\n",
    "FILTERS      = [3,5,7,11]\n",
    "MULTIFILTERS = [[2,3,4], [3,4,5], [7,8,9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbedding(GLOVE_DIR):\n",
    "    embeddings_index = {}\n",
    "    f = open(GLOVE_DIR, encoding=\"utf8\")\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testVersionCNN(embeddings_index, RETRAIN, FILTER, FILTERSIZE,filepath):\n",
    "    embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    embedding_layer = Embedding(len(word_index) + 1,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=RETRAIN)\n",
    "    \n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    \n",
    "    if FILTER:\n",
    "        convs = []\n",
    "        for fsz in FILTERSIZE:\n",
    "            l_conv = Conv1D(filters=128,kernel_size=fsz,activation='relu')(embedded_sequences)\n",
    "            l_pool = MaxPooling1D(5)(l_conv)\n",
    "            convs.append(l_pool)\n",
    "        l_previous = Merge(mode='concat', concat_axis=1)(convs)\n",
    "    else:\n",
    "        l_cov1 = Conv1D(128, FILTERSIZE, activation='relu')(embedded_sequences)\n",
    "        l_previous = MaxPooling1D(5)(l_cov1)\n",
    "    \n",
    "    l_cov2 = Conv1D(128, 5, activation='relu')(l_previous)\n",
    "    l_pool2 = MaxPooling1D(5)(l_cov2)\n",
    "    l_cov3 = Conv1D(128, 5, activation='relu')(l_pool2)\n",
    "    if FILTER:\n",
    "        l_pool3 = MaxPooling1D(int(l_cov3.shape[1]))(l_cov3)  # global max pooling\n",
    "    else:\n",
    "        l_pool3 = MaxPooling1D(int(l_cov3.shape[1]))(l_cov3)\n",
    "    l_flat = Flatten()(l_pool3)\n",
    "    l_drop = Dropout(0.5)(l_flat)\n",
    "    l_dense = Dense(128, activation='relu')(l_drop)\n",
    "    preds = Dense(labelsCNN.shape[1], activation='sigmoid')(l_dense)\n",
    "\n",
    "    modelCNN = Model(sequence_input, preds)\n",
    "    modelCNN.compile(loss='binary_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=[keras.metrics.categorical_accuracy, fmeasure])\n",
    "    filepath= \"best_models\\CNN_\" + filepath\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor=\"val_fmeasure\", verbose=0, save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint]\n",
    "    return modelCNN, callbacks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\PC-Axel\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC-Axel\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 44538 samples, validate on 7859 samples\n",
      "Epoch 1/7\n",
      "44538/44538 [==============================] - 1268s 28ms/step - loss: 0.2830 - categorical_accuracy: 0.2076 - fmeasure: 0.1383 - val_loss: 0.2215 - val_categorical_accuracy: 0.3771 - val_fmeasure: 0.4027\n",
      "Epoch 2/7\n",
      "20224/44538 [============>.................] - ETA: 10:00 - loss: 0.2118 - categorical_accuracy: 0.4135 - fmeasure: 0.4184"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-04c49dc7aab9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m                     \u001b[0mmodelCNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtestVersionCNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRETRAIN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFILTER\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFILTERSIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                     modelCNN.fit(x_train, y_train, validation_data=(x_test, y_test),\n\u001b[1;32m---> 10\u001b[1;33m                                  epochs=7, batch_size=128, callbacks=callbacks_list)\n\u001b[0m\u001b[0;32m     11\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mFILTERSIZE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFILTERS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"3\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"5\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"7\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"11\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1705\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1235\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1236\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2478\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2479\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1140\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1141\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1321\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1312\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[0;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1420\u001b[1;33m             status, run_metadata)\n\u001b[0m\u001b[0;32m   1421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1422\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Without split\n",
    "for RETRAIN,b in zip(TRAINABLE,[\"static_\"]):\n",
    "    for GLOVE_DIR,a in zip(GLOVE_DIRS,[\"W2V_loaded_\",\"W2V_own_\"]):\n",
    "        embeddings_index = getEmbedding(GLOVE_DIR)\n",
    "        for FILTER,c in zip(MULTIFILTER,[\"multi_\",\"single_\"]):\n",
    "            if FILTER:\n",
    "                for FILTERSIZE,d in zip(MULTIFILTERS,[\"234\",\"345\",\"789\"]):\n",
    "                    modelCNN, callbacks_list = testVersionCNN(embeddings_index, RETRAIN, FILTER, FILTERSIZE, a+b+c+d)\n",
    "                    modelCNN.fit(x_train, y_train, validation_data=(x_test, y_test),\n",
    "                                 epochs=7, batch_size=128, callbacks=callbacks_list)\n",
    "            else:\n",
    "                for FILTERSIZE,d in zip(FILTERS,[\"3\",\"5\",\"7\",\"11\"]):\n",
    "                    modelCNN, callbacks_list = testVersionCNN(embeddings_index, RETRAIN, FILTER, FILTERSIZE, a+b+c+d)\n",
    "                    modelCNN.fit(x_train, y_train, validation_data=(x_test, y_test),\n",
    "                                 epochs=7, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(munTexts)\n",
    "\n",
    "munTextsCNN = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = modelCNN.predict(munTextsCNN)\n",
    "\n",
    "for threshold in [.2,.25,.3,.35,.4,.45,.5,.6,.7,.8,.9]:\n",
    "    realPrediction = (np.array(prediction) > threshold).astype(int)\n",
    "    evaluation(yMun,realPrediction)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC-Axel\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\topology.py:1271: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  return cls(**config)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 is 0.23, Precision is 0.134, Recall is 0.79, Accuracy is 0.021\n",
      "F1 is 0.26, Precision is 0.159, Recall is 0.701, Accuracy is 0.03\n",
      "F1 is 0.288, Precision is 0.19, Recall is 0.599, Accuracy is 0.036\n",
      "F1 is 0.338, Precision is 0.26, Recall is 0.483, Accuracy is 0.077\n",
      "F1 is 0.384, Precision is 0.365, Recall is 0.404, Accuracy is 0.144\n",
      "F1 is 0.387, Precision is 0.494, Recall is 0.319, Accuracy is 0.289\n",
      "F1 is 0.365, Precision is 0.592, Recall is 0.263, Accuracy is 0.301\n",
      "F1 is 0.277, Precision is 0.723, Recall is 0.171, Accuracy is 0.273\n",
      "F1 is 0.218, Precision is 0.824, Recall is 0.126, Accuracy is 0.248\n",
      "F1 is 0.147, Precision is 0.857, Recall is 0.081, Accuracy is 0.221\n",
      "F1 is 0.052, Precision is 0.842, Recall is 0.027, Accuracy is 0.18\n",
      "\n",
      "F1 is 0.242, Precision is 0.144, Recall is 0.775, Accuracy is 0.016\n",
      "F1 is 0.261, Precision is 0.163, Recall is 0.653, Accuracy is 0.023\n",
      "F1 is 0.292, Precision is 0.198, Recall is 0.555, Accuracy is 0.036\n",
      "F1 is 0.297, Precision is 0.226, Recall is 0.433, Accuracy is 0.066\n",
      "F1 is 0.345, Precision is 0.343, Recall is 0.347, Accuracy is 0.141\n",
      "F1 is 0.347, Precision is 0.46, Recall is 0.279, Accuracy is 0.257\n",
      "F1 is 0.325, Precision is 0.562, Recall is 0.228, Accuracy is 0.28\n",
      "F1 is 0.249, Precision is 0.748, Recall is 0.149, Accuracy is 0.253\n",
      "F1 is 0.167, Precision is 0.859, Recall is 0.092, Accuracy is 0.214\n",
      "F1 is 0.096, Precision is 0.938, Recall is 0.05, Accuracy is 0.187\n",
      "F1 is 0.036, Precision is 1.0, Recall is 0.018, Accuracy is 0.169\n",
      "\n",
      "F1 is 0.26, Precision is 0.155, Recall is 0.802, Accuracy is 0.007\n",
      "F1 is 0.3, Precision is 0.191, Recall is 0.703, Accuracy is 0.014\n",
      "F1 is 0.318, Precision is 0.223, Recall is 0.554, Accuracy is 0.025\n",
      "F1 is 0.325, Precision is 0.28, Recall is 0.389, Accuracy is 0.068\n",
      "F1 is 0.305, Precision is 0.388, Recall is 0.252, Accuracy is 0.235\n",
      "F1 is 0.274, Precision is 0.502, Recall is 0.188, Accuracy is 0.248\n",
      "F1 is 0.212, Precision is 0.583, Recall is 0.129, Accuracy is 0.226\n",
      "F1 is 0.14, Precision is 0.754, Recall is 0.077, Accuracy is 0.205\n",
      "F1 is 0.095, Precision is 0.811, Recall is 0.05, Accuracy is 0.185\n",
      "F1 is 0.052, Precision is 0.889, Recall is 0.027, Accuracy is 0.173\n",
      "F1 is 0.02, Precision is 1.0, Recall is 0.01, Accuracy is 0.162\n",
      "\n",
      "F1 is 0.235, Precision is 0.138, Recall is 0.784, Accuracy is 0.005\n",
      "F1 is 0.264, Precision is 0.165, Recall is 0.663, Accuracy is 0.016\n",
      "F1 is 0.29, Precision is 0.204, Recall is 0.502, Accuracy is 0.025\n",
      "F1 is 0.273, Precision is 0.223, Recall is 0.352, Accuracy is 0.064\n",
      "F1 is 0.285, Precision is 0.338, Recall is 0.247, Accuracy is 0.232\n",
      "F1 is 0.259, Precision is 0.443, Recall is 0.183, Accuracy is 0.246\n",
      "F1 is 0.23, Precision is 0.521, Recall is 0.148, Accuracy is 0.23\n",
      "F1 is 0.182, Precision is 0.738, Recall is 0.104, Accuracy is 0.228\n",
      "F1 is 0.124, Precision is 0.851, Recall is 0.067, Accuracy is 0.212\n",
      "F1 is 0.077, Precision is 0.923, Recall is 0.04, Accuracy is 0.189\n",
      "F1 is 0.013, Precision is 0.8, Recall is 0.007, Accuracy is 0.159\n",
      "\n",
      "F1 is 0.232, Precision is 0.136, Recall is 0.785, Accuracy is 0.007\n",
      "F1 is 0.263, Precision is 0.162, Recall is 0.696, Accuracy is 0.021\n",
      "F1 is 0.292, Precision is 0.194, Recall is 0.584, Accuracy is 0.039\n",
      "F1 is 0.333, Precision is 0.265, Recall is 0.45, Accuracy is 0.071\n",
      "F1 is 0.34, Precision is 0.331, Recall is 0.351, Accuracy is 0.118\n",
      "F1 is 0.348, Precision is 0.446, Recall is 0.285, Accuracy is 0.278\n",
      "F1 is 0.326, Precision is 0.524, Recall is 0.237, Accuracy is 0.28\n",
      "F1 is 0.264, Precision is 0.671, Recall is 0.164, Accuracy is 0.257\n",
      "F1 is 0.214, Precision is 0.771, Recall is 0.124, Accuracy is 0.239\n",
      "F1 is 0.154, Precision is 0.909, Recall is 0.084, Accuracy is 0.216\n",
      "F1 is 0.074, Precision is 0.958, Recall is 0.039, Accuracy is 0.189\n",
      "\n",
      "F1 is 0.211, Precision is 0.121, Recall is 0.837, Accuracy is 0.007\n",
      "F1 is 0.242, Precision is 0.145, Recall is 0.727, Accuracy is 0.016\n",
      "F1 is 0.267, Precision is 0.177, Recall is 0.54, Accuracy is 0.027\n",
      "F1 is 0.284, Precision is 0.228, Recall is 0.378, Accuracy is 0.055\n",
      "F1 is 0.319, Precision is 0.363, Recall is 0.284, Accuracy is 0.232\n",
      "F1 is 0.302, Precision is 0.482, Recall is 0.22, Accuracy is 0.246\n",
      "F1 is 0.265, Precision is 0.59, Recall is 0.171, Accuracy is 0.251\n",
      "F1 is 0.196, Precision is 0.77, Recall is 0.112, Accuracy is 0.228\n",
      "F1 is 0.13, Precision is 0.857, Recall is 0.07, Accuracy is 0.203\n",
      "F1 is 0.065, Precision is 0.909, Recall is 0.034, Accuracy is 0.18\n",
      "F1 is 0.03, Precision is 1.0, Recall is 0.015, Accuracy is 0.164\n",
      "\n",
      "F1 is 0.215, Precision is 0.125, Recall is 0.762, Accuracy is 0.007\n",
      "F1 is 0.261, Precision is 0.164, Recall is 0.634, Accuracy is 0.021\n",
      "F1 is 0.272, Precision is 0.192, Recall is 0.465, Accuracy is 0.036\n",
      "F1 is 0.276, Precision is 0.239, Recall is 0.326, Accuracy is 0.068\n",
      "F1 is 0.299, Precision is 0.397, Recall is 0.24, Accuracy is 0.241\n",
      "F1 is 0.281, Precision is 0.528, Recall is 0.191, Accuracy is 0.257\n",
      "F1 is 0.239, Precision is 0.593, Recall is 0.149, Accuracy is 0.239\n",
      "F1 is 0.175, Precision is 0.674, Recall is 0.101, Accuracy is 0.221\n",
      "F1 is 0.133, Precision is 0.811, Recall is 0.072, Accuracy is 0.203\n",
      "F1 is 0.083, Precision is 0.867, Recall is 0.044, Accuracy is 0.189\n",
      "F1 is 0.043, Precision is 0.867, Recall is 0.022, Accuracy is 0.171\n",
      "\n",
      "F1 is 0.28, Precision is 0.178, Recall is 0.651, Accuracy is 0.0\n",
      "F1 is 0.274, Precision is 0.186, Recall is 0.52, Accuracy is 0.005\n",
      "F1 is 0.302, Precision is 0.236, Recall is 0.419, Accuracy is 0.018\n",
      "F1 is 0.311, Precision is 0.309, Recall is 0.312, Accuracy is 0.077\n",
      "F1 is 0.235, Precision is 0.404, Recall is 0.166, Accuracy is 0.221\n",
      "F1 is 0.166, Precision is 0.469, Recall is 0.101, Accuracy is 0.21\n",
      "F1 is 0.115, Precision is 0.576, Recall is 0.064, Accuracy is 0.189\n",
      "F1 is 0.08, Precision is 0.806, Recall is 0.042, Accuracy is 0.182\n",
      "F1 is 0.039, Precision is 0.923, Recall is 0.02, Accuracy is 0.169\n",
      "F1 is 0.007, Precision is 0.667, Recall is 0.003, Accuracy is 0.159\n",
      "F1 is 0.007, Precision is 1.0, Recall is 0.003, Accuracy is 0.159\n",
      "\n",
      "F1 is 0.204, Precision is 0.116, Recall is 0.846, Accuracy is 0.005\n",
      "F1 is 0.233, Precision is 0.138, Recall is 0.743, Accuracy is 0.009\n",
      "F1 is 0.258, Precision is 0.166, Recall is 0.586, Accuracy is 0.014\n",
      "F1 is 0.256, Precision is 0.188, Recall is 0.401, Accuracy is 0.027\n",
      "F1 is 0.205, Precision is 0.209, Recall is 0.201, Accuracy is 0.052\n",
      "F1 is 0.166, Precision is 0.332, Recall is 0.111, Accuracy is 0.198\n",
      "F1 is 0.127, Precision is 0.618, Recall is 0.07, Accuracy is 0.196\n",
      "F1 is 0.055, Precision is 0.708, Recall is 0.029, Accuracy is 0.178\n",
      "F1 is 0.033, Precision is 0.833, Recall is 0.017, Accuracy is 0.169\n",
      "F1 is 0.017, Precision is 0.833, Recall is 0.008, Accuracy is 0.164\n",
      "F1 is 0.013, Precision is 1.0, Recall is 0.007, Accuracy is 0.164\n",
      "\n",
      "F1 is 0.178, Precision is 0.099, Recall is 0.874, Accuracy is 0.0\n",
      "F1 is 0.199, Precision is 0.114, Recall is 0.777, Accuracy is 0.0\n",
      "F1 is 0.213, Precision is 0.13, Recall is 0.597, Accuracy is 0.007\n",
      "F1 is 0.247, Precision is 0.17, Recall is 0.446, Accuracy is 0.039\n",
      "F1 is 0.266, Precision is 0.246, Recall is 0.289, Accuracy is 0.091\n",
      "F1 is 0.242, Precision is 0.431, Recall is 0.168, Accuracy is 0.232\n",
      "F1 is 0.143, Precision is 0.557, Recall is 0.082, Accuracy is 0.2\n",
      "F1 is 0.055, Precision is 0.739, Recall is 0.029, Accuracy is 0.173\n",
      "F1 is 0.039, Precision is 1.0, Recall is 0.02, Accuracy is 0.171\n",
      "F1 is 0.013, Precision is 1.0, Recall is 0.007, Accuracy is 0.162\n",
      "F1 is 0.0, Precision is 0.0, Recall is 0.0, Accuracy is 0.155\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC-Axel\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\PC-Axel\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 is 0.2, Precision is 0.112, Recall is 0.894, Accuracy is 0.005\n",
      "F1 is 0.217, Precision is 0.127, Recall is 0.767, Accuracy is 0.007\n",
      "F1 is 0.254, Precision is 0.16, Recall is 0.611, Accuracy is 0.014\n",
      "F1 is 0.29, Precision is 0.217, Recall is 0.438, Accuracy is 0.034\n",
      "F1 is 0.301, Precision is 0.295, Recall is 0.307, Accuracy is 0.114\n",
      "F1 is 0.233, Precision is 0.383, Recall is 0.168, Accuracy is 0.187\n",
      "F1 is 0.155, Precision is 0.478, Recall is 0.092, Accuracy is 0.187\n",
      "F1 is 0.069, Precision is 0.564, Recall is 0.037, Accuracy is 0.178\n",
      "F1 is 0.055, Precision is 0.944, Recall is 0.029, Accuracy is 0.178\n",
      "F1 is 0.023, Precision is 0.875, Recall is 0.012, Accuracy is 0.166\n",
      "F1 is 0.007, Precision is 1.0, Recall is 0.003, Accuracy is 0.159\n",
      "\n",
      "F1 is 0.188, Precision is 0.105, Recall is 0.896, Accuracy is 0.005\n",
      "F1 is 0.212, Precision is 0.123, Recall is 0.763, Accuracy is 0.005\n",
      "F1 is 0.244, Precision is 0.155, Recall is 0.574, Accuracy is 0.009\n",
      "F1 is 0.298, Precision is 0.234, Recall is 0.411, Accuracy is 0.032\n",
      "F1 is 0.283, Precision is 0.408, Recall is 0.216, Accuracy is 0.23\n",
      "F1 is 0.196, Precision is 0.522, Recall is 0.121, Accuracy is 0.205\n",
      "F1 is 0.135, Precision is 0.634, Recall is 0.076, Accuracy is 0.198\n",
      "F1 is 0.064, Precision is 0.69, Recall is 0.034, Accuracy is 0.182\n",
      "F1 is 0.039, Precision is 0.857, Recall is 0.02, Accuracy is 0.173\n",
      "F1 is 0.013, Precision is 0.8, Recall is 0.007, Accuracy is 0.159\n",
      "F1 is 0.007, Precision is 1.0, Recall is 0.003, Accuracy is 0.159\n",
      "\n",
      "F1 is 0.218, Precision is 0.127, Recall is 0.777, Accuracy is 0.009\n",
      "F1 is 0.243, Precision is 0.149, Recall is 0.658, Accuracy is 0.023\n",
      "F1 is 0.277, Precision is 0.188, Recall is 0.529, Accuracy is 0.039\n",
      "F1 is 0.304, Precision is 0.25, Recall is 0.389, Accuracy is 0.068\n",
      "F1 is 0.308, Precision is 0.347, Recall is 0.277, Accuracy is 0.221\n",
      "F1 is 0.27, Precision is 0.432, Recall is 0.196, Accuracy is 0.232\n",
      "F1 is 0.211, Precision is 0.491, Recall is 0.134, Accuracy is 0.214\n",
      "F1 is 0.135, Precision is 0.652, Recall is 0.076, Accuracy is 0.196\n",
      "F1 is 0.074, Precision is 0.793, Recall is 0.039, Accuracy is 0.182\n",
      "F1 is 0.033, Precision is 0.909, Recall is 0.017, Accuracy is 0.173\n",
      "F1 is 0.013, Precision is 1.0, Recall is 0.007, Accuracy is 0.162\n",
      "\n",
      "F1 is 0.212, Precision is 0.121, Recall is 0.841, Accuracy is 0.002\n",
      "F1 is 0.239, Precision is 0.145, Recall is 0.678, Accuracy is 0.005\n",
      "F1 is 0.276, Precision is 0.192, Recall is 0.488, Accuracy is 0.027\n",
      "F1 is 0.266, Precision is 0.25, Recall is 0.284, Accuracy is 0.118\n",
      "F1 is 0.234, Precision is 0.363, Recall is 0.173, Accuracy is 0.198\n",
      "F1 is 0.199, Precision is 0.507, Recall is 0.124, Accuracy is 0.214\n",
      "F1 is 0.168, Precision is 0.604, Recall is 0.097, Accuracy is 0.219\n",
      "F1 is 0.106, Precision is 0.773, Recall is 0.057, Accuracy is 0.196\n",
      "F1 is 0.071, Precision is 0.917, Recall is 0.037, Accuracy is 0.182\n",
      "F1 is 0.03, Precision is 0.9, Recall is 0.015, Accuracy is 0.169\n",
      "F1 is 0.01, Precision is 1.0, Recall is 0.005, Accuracy is 0.159\n",
      "\n",
      "F1 is 0.198, Precision is 0.115, Recall is 0.72, Accuracy is 0.009\n",
      "F1 is 0.237, Precision is 0.147, Recall is 0.609, Accuracy is 0.014\n",
      "F1 is 0.261, Precision is 0.187, Recall is 0.436, Accuracy is 0.036\n",
      "F1 is 0.276, Precision is 0.276, Recall is 0.275, Accuracy is 0.1\n",
      "F1 is 0.245, Precision is 0.386, Recall is 0.18, Accuracy is 0.232\n",
      "F1 is 0.176, Precision is 0.458, Recall is 0.109, Accuracy is 0.214\n",
      "F1 is 0.15, Precision is 0.525, Recall is 0.087, Accuracy is 0.207\n",
      "F1 is 0.1, Precision is 0.727, Recall is 0.054, Accuracy is 0.198\n",
      "F1 is 0.045, Precision is 0.636, Recall is 0.023, Accuracy is 0.18\n",
      "F1 is 0.036, Precision is 0.786, Recall is 0.018, Accuracy is 0.175\n",
      "F1 is 0.013, Precision is 1.0, Recall is 0.007, Accuracy is 0.164\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modelstotest = [\"best_models\\weighted_CNN_W2V_loaded_dynamic_multi_234\", \"best_models\\weighted_CNN_W2V_loaded_dynamic_multi_345\", \"best_models\\weighted_CNN_W2V_loaded_dynamic_multi_789\", \"best_models\\weighted_CNN_W2V_loaded_dynamic_single_11\", \"best_models\\weighted_CNN_W2V_loaded_dynamic_single_3\", \"best_models\\weighted_CNN_W2V_loaded_dynamic_single_5\", \"best_models\\weighted_CNN_W2V_loaded_dynamic_single_7\", \"best_models\\weighted_CNN_W2V_loaded_static_multi_234\", \"best_models\\weighted_CNN_W2V_own_dynamic_multi_234\", \"best_models\\weighted_CNN_W2V_own_dynamic_multi_345\", \"best_models\\weighted_CNN_W2V_own_dynamic_multi_789\", \"best_models\\weighted_CNN_W2V_own_dynamic_single_11\", \"best_models\\weighted_CNN_W2V_own_dynamic_single_3\", \"best_models\\weighted_CNN_W2V_own_dynamic_single_5\", \"best_models\\weighted_CNN_W2V_own_dynamic_single_7\"]\n",
    "for modeltotest in modelstotest:\n",
    "    oldModelCNN= keras.models.load_model(modeltotest, custom_objects={'fmeasure': fmeasure})\n",
    "    prediction = oldModelCNN.predict(munTextsCNN)\n",
    "    for threshold in [.2,.25,.3,.35,.4,.45,.5,.6,.7,.8,.9]:\n",
    "        evaluation(yMun, (prediction >threshold).astype(int))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
