{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter,defaultdict\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ['KERAS_BACKEND']='theano'\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Merge, Dropout\n",
    "from keras.models import Model\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load-In Data/Pre-process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = [\"CNN\", \"SGD\", \"NB\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(article):\n",
    "    article = article.encode('utf-8')\n",
    "    article = str(article.lower())\n",
    "    return removeTagsInterpuntion(article)\n",
    "\n",
    "def removeTagsInterpuntion(article): #remove URL's, HTML-tags and interpuntion\n",
    "    article = re.sub(\"^https?:\\/\\/.*[\\r\\n]*\", '', article)\n",
    "    article = re.sub('\\\\\\\\x\\w\\w', '', article)\n",
    "    article = re.sub('[^a-z\\s]', '', article)\n",
    "    return article\n",
    "\n",
    "def loadData(path, texts, labels, highLevelLabels = True):\n",
    "    with open(path) as data_file:    \n",
    "        data = json.load(data_file)\n",
    "    for article in data:\n",
    "        if highLevelLabels:\n",
    "            label = article[\"category\"].split(\"|\")\n",
    "            labels.append(label[0])\n",
    "        else:\n",
    "            labels.append(article[\"category\"])\n",
    "        texts.append(preprocess(article[\"content\"]))\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "labels = []\n",
    "paths = [r\"C:\\Users\\PC-Axel\\Documents\\github\\thesis\\Data\\PoliFLW Data\\kamerstukken_topics_first.json\",\n",
    "        r\"C:\\Users\\PC-Axel\\Documents\\github\\thesis\\Data\\PoliFLW Data\\kamerstukken_topics_second.json\"]\n",
    "\n",
    "for path in paths:\n",
    "    texts, labels = loadData(path, texts, labels)\n",
    "    \n",
    "\n",
    "#Prepare splitting\n",
    "VALIDATION_SPLIT = 0.2\n",
    "indices = np.arange(len(labels))\n",
    "np.random.shuffle(indices)\n",
    "texts = np.array(texts)[indices]\n",
    "labels = np.array(labels)[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * labels.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 159977 unique tokens.\n",
      "Shape of data tensor: (28294, 1000)\n",
      "Shape of label tensor: (28294, 18)\n",
      "Class distribution in training and validation set \n",
      "[ 242  231  424  234  132  562  182  197    3  350  315  707  327   54\n",
      "  157  356  175 1010]\n"
     ]
    }
   ],
   "source": [
    "if \"CNN\" in testing or \"CNN+\" in testing:\n",
    "    MAX_SEQUENCE_LENGTH = 1000\n",
    "    MAX_NB_WORDS = 20000\n",
    "    EMBEDDING_DIM = 160\n",
    "    \n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    labelsCNN = lb.fit_transform(labels)\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "    print('Shape of data tensor:', data.shape)\n",
    "    print('Shape of label tensor:', labelsCNN.shape)\n",
    "\n",
    "    x_train = data[:-nb_validation_samples]\n",
    "    y_train = labelsCNN[:-nb_validation_samples]\n",
    "    x_val = data[-nb_validation_samples:]\n",
    "    y_val = labelsCNN[-nb_validation_samples:]\n",
    "    \n",
    "    \n",
    "    print('Class distribution in training and validation set ')\n",
    "    print(y_train.sum(axis=0))\n",
    "    print(y_val.sum(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"NB\" in testing or \"SGD\" in testing:\n",
    "    xTrain = texts[:-nb_validation_samples]\n",
    "    yTrain = labels[:-nb_validation_samples]\n",
    "    xVal = texts[-nb_validation_samples:]\n",
    "    yVal = labels[-nb_validation_samples:]\n",
    "    \n",
    "    transformer = TfidfVectorizer(smooth_idf=True, min_df=0.00000001, max_df=0.2, sublinear_tf=True)\n",
    "    \n",
    "    xTrain = transformer.fit_transform(xTrain)\n",
    "    xVal = transformer.transform(xVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(yVal,yPred):\n",
    "    print(\"Accuracy is \" + str(accuracy_score(yVal, yPred)))\n",
    "    print(f1_score(yVal, yPred, average='macro'),f1_score(yVal, yPred, average='micro'), \n",
    "        f1_score(yVal, yPred, average='weighted'))\n",
    "    print(precision_score(yVal, yPred, average='macro'),precision_score(yVal, yPred, average='micro'), \n",
    "        precision_score(yVal, yPred, average='weighted'))\n",
    "    print(recall_score(yVal, yPred, average='macro'),recall_score(yVal, yPred, average='micro'), \n",
    "        recall_score(yVal, yPred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.4536938847649346\n",
      "0.23335726121724246 0.4536938847649346 0.3742497128864561\n",
      "0.6974315367404772 0.4536938847649346 0.687346054243276\n",
      "0.2337264032451163 0.4536938847649346 0.4536938847649346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC-Axel\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\PC-Axel\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "if \"NB\" in testing:\n",
    "    clf = MultinomialNB()\n",
    "    clf = OneVsRestClassifier(clf).fit(xTrain, yTrain)\n",
    "    yPred = clf.predict(xVal)\n",
    "    evaluation(yVal,yPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.8253799929303641\n",
      "0.7604934520356192 0.8253799929303641 0.8237672150883184\n",
      "0.7701495221609246 0.8253799929303641 0.8239666232852711\n",
      "0.7547381229678735 0.8253799929303641 0.8253799929303641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC-Axel\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\PC-Axel\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "if \"SGD\" in testing:\n",
    "    clf = SGDClassifier(loss='log', penalty='l1', alpha=1e-6, random_state=42, max_iter=10, tol=None)\n",
    "    clf = OneVsRestClassifier(clf).fit(xTrain, yTrain)\n",
    "    yPred = clf.predict(xVal)\n",
    "    evaluation(yVal,yPred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models and evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_f1s = []\n",
    "        self.val_recalls = []\n",
    "        self.val_precisions = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        val_predict = (np.asarray(self.model.predict(self.model.validation_data[0]))).round()\n",
    "        val_targ = self.model.validation_data[1]\n",
    "        _val_f1 = f1_score(val_targ, val_predict)\n",
    "        _val_recall = recall_score(val_targ, val_predict)\n",
    "        _val_precision = precision_score(val_targ, val_predict)\n",
    "        self.val_f1s.append(_val_f1)\n",
    "        self.val_recalls.append(_val_recall)\n",
    "        self.val_precisions.append(_val_precision)\n",
    "        print(\"— val_f1: %f — val_precision: %f — val_recall %f\" %(_val_f1, _val_precision, _val_recall))\n",
    "        return\n",
    "\n",
    "metrics = Metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 1442951 word vectors in Glove 6B 100d.\n"
     ]
    }
   ],
   "source": [
    "if \"CNN\" in testing or \"CNN+\" in testing:\n",
    "    GLOVE_DIR = r\"C:\\Users\\PC-Axel\\Documents\\Codeer projecten\\Word2Vec Vectoren\\Nederlandse word2vec\\combined-160.txt\"\n",
    "    embeddings_index = {}\n",
    "    f = open(GLOVE_DIR, encoding=\"utf8\")\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "\n",
    "    print('Total %s word vectors in Glove 6B 100d.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"CNN\" in testing:\n",
    "    embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    embedding_layer = Embedding(len(word_index) + 1,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=True)\n",
    "\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    l_cov1= Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "    l_pool1 = MaxPooling1D(5)(l_cov1)\n",
    "    l_cov2 = Conv1D(128, 5, activation='relu')(l_pool1)\n",
    "    l_pool2 = MaxPooling1D(5)(l_cov2)\n",
    "    l_cov3 = Conv1D(128, 5, activation='relu')(l_pool2)\n",
    "    l_pool3 = MaxPooling1D(35)(l_cov3)  # global max pooling\n",
    "    l_flat = Flatten()(l_pool3)\n",
    "    l_dense = Dense(128, activation='relu')(l_flat)\n",
    "    preds = Dense(labelsCNN.shape[1], activation='softmax')(l_dense)\n",
    "\n",
    "    modelCNN = Model(sequence_input, preds)\n",
    "    modelCNN.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if \"CNN\" in testing:\n",
    "    print(\"modelCNN fitting - simplified convolutional neural network\")\n",
    "    modelCNN.summary()\n",
    "    modelCNN.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "              epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"CNN+\" in testing:\n",
    "    embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    embedding_layer = Embedding(len(word_index) + 1,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=True)\n",
    "\n",
    "    # applying a more complex convolutional approach\n",
    "    convs = []\n",
    "    filter_sizes = [3,4,5]\n",
    "\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    for fsz in filter_sizes:\n",
    "        l_conv = Conv1D(nb_filter=128,filter_length=fsz,activation='relu')(embedded_sequences)\n",
    "        l_pool = MaxPooling1D(5)(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    l_merge = Merge(mode='concat', concat_axis=1)(convs)\n",
    "    l_cov1= Conv1D(128, 5, activation='relu')(l_merge)\n",
    "    l_pool1 = MaxPooling1D(5)(l_cov1)\n",
    "    l_cov2 = Conv1D(128, 5, activation='relu')(l_pool1)\n",
    "    l_pool2 = MaxPooling1D(30)(l_cov2)\n",
    "    l_flat = Flatten()(l_pool2)\n",
    "    l_dense = Dense(128, activation='relu')(l_flat)\n",
    "    preds = Dense(labelsCNN.shape[1], activation='softmax')(l_dense)\n",
    "\n",
    "    modelCNNPlus = Model(sequence_input, preds)\n",
    "    modelCNNPlus.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"CNN+\" in testing:\n",
    "    print(\"modelCNNPlus fitting - more complex convolutional neural network\")\n",
    "    modelCNNPlus.summary()\n",
    "    modelCNNPlus.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "              nb_epoch=20, batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
