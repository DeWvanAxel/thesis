{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC-Axel\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\PC-Axel\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter,defaultdict\n",
    "import csv\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import os\n",
    "import ast\n",
    "from gensim.models.doc2vec import Doc2Vec,LabeledSentence\n",
    "\n",
    "#os.environ['KERAS_BACKEND']='theano'\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Merge, Dropout\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import Callback\n",
    "import keras.objectives\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score, classification_report,jaccard_similarity_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "labels = []\n",
    "urls = []\n",
    "\n",
    "with open('parlementData_2001-2017.csv', 'r') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter='&',quotechar='|')\n",
    "    for row in spamreader:\n",
    "        urls.append(row[0])\n",
    "        texts.append(row[1])\n",
    "        labels.append(ast.literal_eval(row[2]))\n",
    "        \n",
    "munTexts = []\n",
    "munLabels = []\n",
    "munNumber = []\n",
    "\n",
    "with open('municipality.csv', 'r') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter='&',quotechar='|')\n",
    "    for row in spamreader:\n",
    "        munNumber.append(row[0])\n",
    "        munTexts.append(row[1])\n",
    "        munLabels.append(ast.literal_eval(row[2]))\n",
    "        \n",
    "with open('municipalityFam.csv', 'r') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter='&',quotechar='|')\n",
    "    for row in spamreader:\n",
    "        munNumber.append(row[0])\n",
    "        munTexts.append(row[1])\n",
    "        munLabels.append(ast.literal_eval(row[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 392519 unique tokens.\n",
      "Shape of data tensor: (52397, 1000)\n",
      "Shape of label tensor: (52397, 17)\n",
      "Class distribution in training and validation set \n",
      "[2916 3559 6695 3454 1079 6047 1815 3192 4703 2935 8692 8222 1637 1591\n",
      " 3651 2902 9145]\n",
      "[ 523  632 1206  591  181 1070  330  513  873  574 1557 1437  257  294\n",
      "  698  519 1595]\n"
     ]
    }
   ],
   "source": [
    "TEST_SET = 0.15\n",
    "nb_test_samples = int(TEST_SET * len(labels))\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 160\n",
    "\n",
    "lb = MultiLabelBinarizer()\n",
    "labelsCNN = lb.fit_transform(labels)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labelsCNN.shape)\n",
    "\n",
    "x_train = data[:-nb_test_samples]\n",
    "y_train = labelsCNN[:-nb_test_samples]\n",
    "x_test = data[-nb_test_samples:]\n",
    "y_test = labelsCNN[-nb_test_samples:]\n",
    "\n",
    "xTrain = texts[:-nb_test_samples]\n",
    "xTest = texts[-nb_test_samples:]\n",
    "\n",
    "transformer = TfidfVectorizer(smooth_idf=False, min_df=0.00001, max_df=0.2, sublinear_tf=True)\n",
    "    \n",
    "xTrain = transformer.fit_transform(xTrain)\n",
    "xTest = transformer.transform(xTest)\n",
    "\n",
    "print('Class distribution in training and validation set ')\n",
    "print(y_train.sum(axis=0))\n",
    "print(y_test.sum(axis=0))\n",
    "\n",
    "xMun = transformer.transform(munTexts)\n",
    "yMun = lb.transform(munLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(yVal,yPred):\n",
    "    print(\"F1 is \"        + str(round(f1_score(yVal, yPred, average=\"micro\"),3)) + \n",
    "        \", Precision is \" + str(round(precision_score(yVal, yPred, average=\"micro\"),3)) +\n",
    "        \", Recall is \"    + str(round(recall_score(yVal, yPred, average=\"micro\"),3)) +\n",
    "        \", Accuracy is \"  + str(round(accuracy_score(yVal, yPred),3)))\n",
    "\n",
    "\n",
    "def evaluationWithBestThreshold(clf, xMun, yMun, Thresholds = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]):\n",
    "    bestF1 = 0\n",
    "    bestThreshold = 0\n",
    "    yPred = clf.predict_proba(xMun)\n",
    "    for threshold in [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]:\n",
    "        F1 = f1_score(yMun, (yPred > threshold).astype(int), average=\"micro\")\n",
    "        if F1 > bestF1: \n",
    "            bestF1 = F1\n",
    "            bestThreshold = threshold\n",
    "    evaluation(yMun,(yPred > bestThreshold).astype(int))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BASELINES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes - Parliament\n",
      "F1 is 0.283, Precision is 0.892, Recall is 0.168, Accuracy is 0.158\n",
      "\n",
      " Naive Bayes - Municipality\n",
      "F1 is 0.037, Precision is 0.085, Recall is 0.024, Accuracy is 0.307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC-Axel\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB(fit_prior= False)\n",
    "clf = OneVsRestClassifier(clf).fit(xTrain, y_train)\n",
    "\n",
    "print(\"Naive Bayes - Parliament\")\n",
    "evaluationWithBestThreshold(clf, xTest, y_test)\n",
    "\n",
    "print(\"\\n Naive Bayes - Municipality\")\n",
    "evaluationWithBestThreshold(clf, xMun, yMun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SGDClassifier(alpha = 1e-05, loss = 'hinge', penalty = 'l2', random_state=42, max_iter=10, tol=None)\n",
    "clf = OneVsRestClassifier(clf).fit(xTrain, y_train)\n",
    "\n",
    "print(\"SVM - Parliament\")\n",
    "yPred = clf.predict(xTest)\n",
    "evaluation(y_test,yPred)\n",
    "\n",
    "print(\"\\n SVM - Municipality\")\n",
    "yPred = clf.predict(xMun)\n",
    "evaluation(yMun,yPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Parliament\n",
      "F1 is 0.78, Precision is 0.781, Recall is 0.7789, Accuracy is 0.5025\n",
      "\n",
      " Logistic Regression - Municipality\n",
      "F1 is 0.2781, Precision is 0.2226, Recall is 0.3706, Accuracy is 0.1376\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C= 10, penalty= 'l2', solver= 'liblinear',dual=False,max_iter=250)\n",
    "clf = OneVsRestClassifier(clf).fit(xTrain, y_train)\n",
    "\n",
    "print(\"Logistic Regression - Parliament\")\n",
    "evaluationWithBestThreshold(clf, xTest, y_test)\n",
    "\n",
    "print(\"\\n Logistic Regression - Municipality\")\n",
    "evaluationWithBestThreshold(clf, xMun, yMun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(criterion= 'entropy', max_depth= 100, n_estimators= 30)\n",
    "clf = OneVsRestClassifier(clf).fit(xTrain, y_train)\n",
    "\n",
    "print(\"Random Forest - Parliament\")\n",
    "evaluationWithBestThreshold(clf, xTest, y_test)\n",
    "\n",
    "print(\"\\n Random Forest - Municipality\")\n",
    "evaluationWithBestThreshold(clf, xTest, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KIM'S CNN + SPLITUPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "W2V:                Self-trained, imported\n",
    "Embedding:          Static, Dynamic\n",
    "split:              true, false\n",
    "multi-filter sizes: true, false\n",
    "filter sizes:       3,5,7,11\n",
    "filter sizes:       [2,3,4], [3,4,5], [7,8,9]\n",
    "\n",
    "2 * 2 * 2 * 2 * 4 + 2 * 2 * 2 * 2 * 3 = 112 versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_DIRS   = [r\"C:\\Users\\PC-Axel\\Documents\\Codeer projecten\\Word2Vec Vectoren\\Nederlandse word2vec\\combined-160.txt\",\n",
    "                r\"C:\\Users\\PC-Axel\\Documents\\GitHub\\thesis\\Code\\Data preparation\\zelfgemaakte-w2v.txt\"]\n",
    "TRAINABLE    = [True, False]\n",
    "SPLIT        = [True, False]\n",
    "MULTIFILTER  = [True, False]\n",
    "FILTERS      = [3,5,7,11]\n",
    "MULTIFILTERS = [[2,3,4], [3,4,5], [7,8,9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmeasure(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "    p = true_positives / (predicted_positives + K.epsilon())\n",
    "    r = true_positives / (possible_positives + K.epsilon())\n",
    "\n",
    "    beta = 1 # fmeasure\n",
    "    bb = beta**2\n",
    "    fbeta_score = (1 + bb) * (p * r) / (bb * p + r + K.epsilon())\n",
    "    return fbeta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbedding(GLOVE_DIR):\n",
    "    embeddings_index = {}\n",
    "    f = open(GLOVE_DIR, encoding=\"utf8\")\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testVersionCNN(embeddings_index, RETRAIN, FILTER, FILTERSIZE,filepath):\n",
    "    embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    embedding_layer = Embedding(len(word_index) + 1,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=RETRAIN)\n",
    "    \n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    \n",
    "    if FILTER:\n",
    "        convs = []\n",
    "        for fsz in FILTERSIZE:\n",
    "            l_conv = Conv1D(filters=128,kernel_size=fsz,activation='relu')(embedded_sequences)\n",
    "            l_pool = MaxPooling1D(5)(l_conv)\n",
    "            convs.append(l_pool)\n",
    "        l_previous = Merge(mode='concat', concat_axis=1)(convs)\n",
    "    else:\n",
    "        l_cov1 = Conv1D(128, FILTERSIZE, activation='relu')(embedded_sequences)\n",
    "        l_previous = MaxPooling1D(5)(l_cov1)\n",
    "    \n",
    "    l_cov2 = Conv1D(128, 5, activation='relu')(l_previous)\n",
    "    l_pool2 = MaxPooling1D(5)(l_cov2)\n",
    "    l_cov3 = Conv1D(128, 5, activation='relu')(l_pool2)\n",
    "    if FILTER:\n",
    "        l_pool3 = MaxPooling1D(int(l_cov3.shape[1]))(l_cov3)  # global max pooling\n",
    "    else:\n",
    "        l_pool3 = MaxPooling1D(int(l_cov3.shape[1]))(l_cov3)\n",
    "    l_flat = Flatten()(l_pool3)\n",
    "    l_drop = Dropout(0.5)(l_flat)\n",
    "    l_dense = Dense(128, activation='relu')(l_drop)\n",
    "    preds = Dense(labelsCNN.shape[1], activation='sigmoid')(l_dense)\n",
    "\n",
    "    modelCNN = Model(sequence_input, preds)\n",
    "    modelCNN.compile(loss='binary_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=[keras.metrics.categorical_accuracy, fmeasure])\n",
    "    filepath= \"best_models\\CNN_\" + filepath\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor=\"val_fmeasure\", verbose=0, save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint]\n",
    "    return modelCNN, callbacks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC-Axel\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 44538 samples, validate on 7859 samples\n",
      "Epoch 1/10\n",
      "44538/44538 [==============================] - 1144s 26ms/step - loss: 0.2844 - categorical_accuracy: 0.2036 - fmeasure: 0.1322 - val_loss: 0.2285 - val_categorical_accuracy: 0.3675 - val_fmeasure: 0.4155\n",
      "\n",
      "Epoch 00001: val_fmeasure improved from -inf to 0.41549, saving model to best_models\\CNN_W2V_loaded_dynamic_multi_234\n",
      "Epoch 2/10\n",
      "32128/44538 [====================>.........] - ETA: 4:59 - loss: 0.2072 - categorical_accuracy: 0.4263 - fmeasure: 0.4392"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-3c660fbea789>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m                     \u001b[0mmodelCNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtestVersionCNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRETRAIN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFILTER\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFILTERSIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                     modelCNN.fit(x_train, y_train, validation_data=(x_test, y_test),\n\u001b[1;32m---> 10\u001b[1;33m                                  epochs=10, batch_size=128, callbacks=callbacks_list)\n\u001b[0m\u001b[0;32m     11\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mFILTERSIZE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFILTERS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"3\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"5\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"7\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"11\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1705\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1235\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1236\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2478\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2479\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1140\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1141\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1321\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1312\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[0;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1420\u001b[1;33m             status, run_metadata)\n\u001b[0m\u001b[0;32m   1421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1422\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Without split\n",
    "for GLOVE_DIR,a in zip(GLOVE_DIRS,[\"W2V_loaded_\",\"W2V_own_\"]):\n",
    "    embeddings_index = getEmbedding(GLOVE_DIR)\n",
    "    for RETRAIN,b in zip(TRAINABLE,[\"dynamic_\", \"static_\"]):\n",
    "        for FILTER,c in zip(MULTIFILTER,[\"multi_\",\"single_\"]):\n",
    "            if FILTER:\n",
    "                for FILTERSIZE,d in zip(MULTIFILTERS,[\"234\",\"345\",\"789\"]):\n",
    "                    modelCNN, callbacks_list = testVersionCNN(embeddings_index, RETRAIN, FILTER, FILTERSIZE, a+b+c+d)\n",
    "                    modelCNN.fit(x_train, y_train, validation_data=(x_test, y_test),\n",
    "                                 epochs=10, batch_size=128, callbacks=callbacks_list)\n",
    "            else:\n",
    "                for FILTERSIZE,d in zip(FILTERS,[\"3\",\"5\",\"7\",\"11\"]):\n",
    "                    modelCNN, callbacks_list = testVersionCNN(embeddings_index, RETRAIN, FILTER, FILTERSIZE, a+b+c+d)\n",
    "                    modelCNN.fit(x_train, y_train, validation_data=(x_test, y_test),\n",
    "                                 epochs=10, batch_size=128, callbacks=callbacks_list)\n",
    "# With split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PAR2VEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KALCHBRENNER CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
