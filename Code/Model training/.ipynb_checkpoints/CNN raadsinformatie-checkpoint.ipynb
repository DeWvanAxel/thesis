{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import json\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ['KERAS_BACKEND']='theano'\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Merge, Dropout\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 160\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"\\\\\", \"\", string.decode(\"utf-8\"))    \n",
    "    string = re.sub(r\"\\'\", \"\", string)    \n",
    "    string = re.sub(r\"\\\"\", \"\", string)    \n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(article):\n",
    "    article = article.encode('utf-8')\n",
    "    article = str(article.lower())\n",
    "    article = remove_html_tags(article)\n",
    "    article = re.sub('\\.', ' \\. ', article)\n",
    "    article = re.sub('[^a-z\\.\\s]', '', str(article))\n",
    "    return article\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    \"\"\"Remove html tags from a string\"\"\"\n",
    "    text = re.sub('^https?:\\/\\/.*[\\r\\n]*', '', text)\n",
    "    clean = re.compile('<.*?>')\n",
    "    text = re.sub(clean, '', text)\n",
    "    clean = re.compile('\\\\\\\\x\\w\\w')\n",
    "    return re.sub(clean, '', text)\n",
    "\n",
    "path = r\"C:\\Users\\PC-Axel\\Documents\\github\\thesis\\Data\\PoliFLW Data\\kamerstukken_topics_first.json\"\n",
    "\n",
    "with open(path) as data_file:    \n",
    "    data = json.load(data_file)\n",
    "\n",
    "texts = []    \n",
    "labels = []\n",
    "\n",
    "\n",
    "for article in data:\n",
    "    category = article[\"category\"]#.split(\"|\")\n",
    "    labels.append(category)#[0])\n",
    "    texts.append(preprocess(article[\"content\"]))\n",
    "    \n",
    "path = r\"C:\\Users\\PC-Axel\\Documents\\github\\thesis\\Data\\PoliFLW Data\\kamerstukken_topics_second.json\"\n",
    "\n",
    "with open(path) as data_file:    \n",
    "    testData = json.load(data_file)\n",
    "    \n",
    "for article in testData:\n",
    "    category = article[\"category\"]#.split(\"|\")\n",
    "    labels.append(category)#[0])\n",
    "    texts.append(preprocess(article[\"content\"]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28294, 112)\n"
     ]
    }
   ],
   "source": [
    "lb = preprocessing.LabelBinarizer()\n",
    "labels = lb.fit_transform(labels)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 160165 unique tokens.\n",
      "Shape of data tensor: (28294, 1000)\n",
      "Shape of label tensor: (28294, 112)\n",
      "Class distribution in traing and validation set \n",
      "[ 151  345   48  245   69   22  115   24   87   27  194   38   42  312\n",
      "  215   53  286  234   68  132  218  163  579    9   50   16  449  228\n",
      "   83  244   61  224   77  130  405  305  198   22  182 1113   10  367\n",
      "  197   52   41    8  554   53   12  261   11    6  124   70  380   18\n",
      "   75  367  172  200   61  144  150  270   50  399   72  137  785 1150\n",
      "  493   37  119  252    7   12  195  116  213  443  354   23   74   22\n",
      "   56  113   32    5  253  166   54   53  198  132  325  150  450  127\n",
      "  170    1   73  206  133  116  251  214  405 2182  143  324   80  205]\n",
      "[ 38  86  14  49  17   6  21   6  24   6  49  13  10  63  40   8  82  63\n",
      "  21  27  64  37 136   2  11   4 119  61  28  65  12  56  17  35  89  71\n",
      "  29   5  54 304   1 113  48  17  10   0 146  14   4  64   4   2  41  13\n",
      "  97   4  17  84  50  51   9  35  36  67  12  90  19  29 186 290 139  12\n",
      "  42  68   3   2  65  29  66 102  87   6  14   7  11  36   4   2  65  45\n",
      "   8  12  57  31  88  35 120  21  33   1   8  61  35  38  65  57 100 504\n",
      "  31  77  24  52]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "#labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]\n",
    "\n",
    "print('Class distribution in traing and validation set ')\n",
    "print(y_train.sum(axis=0))\n",
    "print(y_val.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 1442951 word vectors in Glove 6B 100d.\n"
     ]
    }
   ],
   "source": [
    "GLOVE_DIR = r\"C:\\Users\\PC-Axel\\Documents\\Codeer projecten\\Word2Vec Vectoren\\Nederlandse word2vec\\combined-160.txt\"\n",
    "embeddings_index = {}\n",
    "f = open(GLOVE_DIR, encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Total %s word vectors in Glove 6B 100d.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "l_cov1= Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "l_pool1 = MaxPooling1D(5)(l_cov1)\n",
    "l_cov2 = Conv1D(128, 5, activation='relu')(l_pool1)\n",
    "l_pool2 = MaxPooling1D(5)(l_cov2)\n",
    "l_cov3 = Conv1D(128, 5, activation='relu')(l_pool2)\n",
    "l_pool3 = MaxPooling1D(35)(l_cov3)  # global max pooling\n",
    "l_flat = Flatten()(l_pool3)\n",
    "l_dense = Dense(128, activation='relu')(l_flat)\n",
    "preds = Dense(112, activation='softmax')(l_dense)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - simplified convolutional neural network\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 1000, 160)         25626560  \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 996, 128)          102528    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 199, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 195, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 39, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 35, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 1, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 112)               14448     \n",
      "=================================================================\n",
      "Total params: 25,924,144\n",
      "Trainable params: 25,924,144\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 22636 samples, validate on 5658 samples\n",
      "Epoch 1/10\n",
      "22636/22636 [==============================] - 862s 38ms/step - loss: 4.0208 - acc: 0.1281 - val_loss: 3.5595 - val_acc: 0.1812\n",
      "Epoch 2/10\n",
      "22636/22636 [==============================] - 1550s 68ms/step - loss: 3.3199 - acc: 0.2062 - val_loss: 3.1213 - val_acc: 0.2485\n",
      "Epoch 3/10\n",
      "22636/22636 [==============================] - 467s 21ms/step - loss: 2.7680 - acc: 0.3006 - val_loss: 2.6323 - val_acc: 0.3432\n",
      "Epoch 4/10\n",
      "22636/22636 [==============================] - 460s 20ms/step - loss: 2.3214 - acc: 0.4012 - val_loss: 2.3484 - val_acc: 0.4031\n",
      "Epoch 5/10\n",
      "22636/22636 [==============================] - 462s 20ms/step - loss: 1.9766 - acc: 0.4743 - val_loss: 2.3427 - val_acc: 0.4189\n",
      "Epoch 6/10\n",
      "22636/22636 [==============================] - 462s 20ms/step - loss: 1.6902 - acc: 0.5421 - val_loss: 2.3678 - val_acc: 0.4221\n",
      "Epoch 7/10\n",
      "22636/22636 [==============================] - 466s 21ms/step - loss: 1.4274 - acc: 0.6088 - val_loss: 2.2548 - val_acc: 0.4684\n",
      "Epoch 8/10\n",
      "22636/22636 [==============================] - 465s 21ms/step - loss: 1.1793 - acc: 0.6728 - val_loss: 2.1395 - val_acc: 0.5145\n",
      "Epoch 9/10\n",
      "22636/22636 [==============================] - 459s 20ms/step - loss: 0.9551 - acc: 0.7309 - val_loss: 2.2964 - val_acc: 0.5168\n",
      "Epoch 10/10\n",
      "22636/22636 [==============================] - 471s 21ms/step - loss: 0.7737 - acc: 0.7832 - val_loss: 2.3630 - val_acc: 0.5053\n"
     ]
    }
   ],
   "source": [
    "print(\"model fitting - simplified convolutional neural network\")\n",
    "model.summary()\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "\n",
    "# applying a more complex convolutional approach\n",
    "convs = []\n",
    "filter_sizes = [3,4,5]\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "for fsz in filter_sizes:\n",
    "    l_conv = Conv1D(nb_filter=128,filter_length=fsz,activation='relu')(embedded_sequences)\n",
    "    l_pool = MaxPooling1D(5)(l_conv)\n",
    "    convs.append(l_pool)\n",
    "    \n",
    "l_merge = Merge(mode='concat', concat_axis=1)(convs)\n",
    "l_cov1= Conv1D(128, 5, activation='relu')(l_merge)\n",
    "l_pool1 = MaxPooling1D(5)(l_cov1)\n",
    "l_cov2 = Conv1D(128, 5, activation='relu')(l_pool1)\n",
    "l_pool2 = MaxPooling1D(30)(l_cov2)\n",
    "l_flat = Flatten()(l_pool2)\n",
    "l_dense = Dense(128, activation='relu')(l_flat)\n",
    "preds = Dense(2, activation='softmax')(l_dense)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(\"model fitting - more complex convolutional neural network\")\n",
    "model.summary()\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          nb_epoch=20, batch_size=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
